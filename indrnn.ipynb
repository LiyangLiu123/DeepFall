{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "indrnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiyangLiu123/DeepFall/blob/master/indrnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CeK_B3f9uxed",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# def train_opts(parser):\n",
        "#   parser.add_argument('--lr', type=float, default=2e-4,help='lr')\n",
        "#   parser.add_argument('--batch_size', type=int, default=128, help='batch_size')\n",
        "#   parser.add_argument('--seq_len', type=int, default=20)\n",
        "#   parser.add_argument('--num_layers', type=int, default=6,help='num_layers')\n",
        "#   parser.add_argument('--hidden_size', type=int, default=512)\n",
        "#   parser.add_argument('--test_CV', action='store_true', default=False,help='use the CS test setting. If True, then use CV test setting.')\n",
        "#   parser.add_argument('--use_weightdecay_nohiddenW', action='store_true', default=False)\n",
        "#   parser.add_argument('--decayfactor', type=float, default=1e-4,help='lr')\n",
        "#   parser.add_argument('--opti', type=str, default='adam')\n",
        "#   parser.add_argument('--pThre', type=int, default=20)\n",
        "#   parser.add_argument('--test_no', type=int, default=20)\n",
        "\n",
        "#   parser.add_argument('--ini_in2hid', type=float, default=0.002)\n",
        "\n",
        "#   parser.add_argument('--constrain_U', action='store_true', default=False)\n",
        "#   parser.add_argument('--MAG', type=float, default=5.0)\n",
        "\n",
        "#   parser.add_argument('--eval_fold', type=int, default=5)\n",
        "#   parser.add_argument('--use_bneval', action='store_true', default=False)\n",
        "#   parser.add_argument('--ini_b', type=float, default=0.0)\n",
        "#   parser.add_argument('--end_rate', type=float, default=1e-6)\n",
        "  \n",
        "#   parser.add_argument('--dropout', type=float, default=0.1,help='lr')\n",
        "  \n",
        "lr = 2e-4\n",
        "batch_size = 128\n",
        "seq_len = 20\n",
        "num_layers = 6\n",
        "hidden_size = 512\n",
        "test_CV = False\n",
        "use_weightdecay_nohiddenW = False\n",
        "decay_factor = 1e-4\n",
        "opti = 'adam'\n",
        "pThre = 20\n",
        "global_test_no = 20\n",
        "ini_in2hid = 0.002\n",
        "constrain_U = False\n",
        "MAG = 5.0\n",
        "eval_fold = 5\n",
        "use_bneval = False\n",
        "ini_b = 0.0\n",
        "end_rate = 1e-6\n",
        "\n",
        "if test_CV:\n",
        "  dropout = 0.1\n",
        "else:\n",
        "  dropout = 0.25\n",
        "  \n",
        "n_dimension = 3\n",
        "num_joints = 50 # two skeletons with each 25 joints\n",
        "outputclass = 60\n",
        "#epochs_to_train = 100\n",
        "\n",
        "if test_CV:\n",
        "  train_link = 'https://drive.google.com/open?id=123bqiqgT6uh2bnOXVBLQqh9nqHWBuv3c'\n",
        "  train_len_link = 'https://drive.google.com/open?id=14ME-N1llC8LnDi5wTOhLl21UFl9Mflkj'\n",
        "  \n",
        "  test_link = 'https://drive.google.com/open?id=1ek_xIGF3kYLsBNrz341otoAvcYPSax3a'\n",
        "  test_len_link = 'https://drive.google.com/open?id=1oR7pVvSlp3i82DKUxBXFyFaJmayxGwIu'\n",
        "  \n",
        "  if outputclass is 60:\n",
        "    train_label_link = 'https://drive.google.com/open?id=1uPK9_8Ml4ktFaF2aMctPehNTM1QIEiFw'\n",
        "    test_label_link = 'https://drive.google.com/open?id=1f1B_4DOdIQsEBYXxVllKezyWh_zUyaj9'\n",
        "  else:\n",
        "    train_label_link = 'https://drive.google.com/open?id=18Tvy4o97TEKjgXEcz8qRL6POt92n-7m1'\n",
        "    test_label_link = 'https://drive.google.com/open?id=16vZ-LHW4rFs4Tgvs9UoK8cgpMKWRWgDY'\n",
        "else:\n",
        "\n",
        "  train_link = 'https://drive.google.com/open?id=199wLS6SYhLwAXumTY483ACoqsaQ_ADmP'\n",
        "  train_len_link = 'https://drive.google.com/open?id=1alMuIeD-1wjAlsJFPfAm_7RiNCJ4-KY-'\n",
        "\n",
        "  test_link = 'https://drive.google.com/open?id=199wLS6SYhLwAXumTY483ACoqsaQ_ADmP'\n",
        "  test_len_link = 'https://drive.google.com/open?id=1alMuIeD-1wjAlsJFPfAm_7RiNCJ4-KY-'\n",
        "  \n",
        "  if outputclass is 60:\n",
        "    train_label_link = 'https://drive.google.com/open?id=1BILzI1hUwtMEd0RUia6MCr3towN5TxU3'\n",
        "    test_label_link = 'https://drive.google.com/open?id=1BILzI1hUwtMEd0RUia6MCr3towN5TxU3'\n",
        "  else:\n",
        "    train_label_link = 'https://drive.google.com/open?id=1xx9KvsUerVIG9IdPo-bKNbsdMI-f8O2K'\n",
        "    test_label_link = 'https://drive.google.com/open?id=1DCiUG5ymsuJFLocdX4WNKQY-7xiVIqtw'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "il_6gDwCu3PI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This code is to implement the IndRNN (only the recurrent part). The code is based on the implementation from \n",
        "https://github.com/StefOe/indrnn-pytorch/blob/master/indrnn.py.\n",
        "Since this only contains the recurrent part of IndRNN, fully connected layers or convolutional layers are needed before it.\n",
        "Please cite the following paper if you find it useful.\n",
        "Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. \"Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN,\" \n",
        "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5457-5466. 2018.\n",
        "@inproceedings{li2018independently,\n",
        "  title={Independently recurrent neural network (indrnn): Building A longer and deeper RNN},\n",
        "  author={Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},\n",
        "  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n",
        "  pages={5457--5466},\n",
        "  year={2018}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import Parameter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "\n",
        "\n",
        "class IndRNNCell_onlyrecurrent(nn.Module):\n",
        "    r\"\"\"An IndRNN cell with ReLU non-linearity. This is only the recurrent part where the input is already processed with w_{ih} * x + b_{ih}.\n",
        "\n",
        "    .. math::\n",
        "        input=w_{ih} * x + b_{ih}\n",
        "        h' = \\relu(input +  w_{hh} (*) h)\n",
        "    With (*) being element-wise vector multiplication.\n",
        "\n",
        "    Args:\n",
        "        hidden_size: The number of features in the hidden state h\n",
        "\n",
        "    Inputs: input, hidden\n",
        "        - **input** (batch, input_size): tensor containing input features\n",
        "        - **hidden** (batch, hidden_size): tensor containing the initial hidden\n",
        "          state for each element in the batch.\n",
        "\n",
        "    Outputs: h'\n",
        "        - **h'** (batch, hidden_size): tensor containing the next hidden state\n",
        "          for each element in the batch\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, \n",
        "                 hidden_max_abs=None, recurrent_init=None):\n",
        "        super(IndRNNCell_onlyrecurrent, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.recurrent_init = recurrent_init\n",
        "        self.weight_hh = Parameter(torch.Tensor(hidden_size))            \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for name, weight in self.named_parameters():\n",
        "            if \"weight_hh\" in name:\n",
        "                if self.recurrent_init is None:\n",
        "                    nn.init.uniform(weight, a=0, b=1)\n",
        "                else:\n",
        "                    self.recurrent_init(weight)\n",
        "\n",
        "    def forward(self, input, hx):\n",
        "        return F.relu(input + hx * self.weight_hh.unsqueeze(0).expand(hx.size(0), len(self.weight_hh)))\n",
        "\n",
        "\n",
        "class IndRNN_onlyrecurrent(nn.Module):\n",
        "    r\"\"\"Applies an IndRNN with `ReLU` non-linearity to an input sequence. \n",
        "    This is only the recurrent part where the input is already processed with w_{ih} * x + b_{ih}.\n",
        "\n",
        "\n",
        "    For each element in the input sequence, each layer computes the following\n",
        "    function:\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        h_t = \\relu(input_t +  w_{hh} (*) h_{(t-1)})\n",
        "\n",
        "    where :math:`h_t` is the hidden state at time `t`, and :math:`input_t`\n",
        "    is the input at time `t`. (*) is element-wise multiplication.\n",
        "\n",
        "    Args:\n",
        "        hidden_size: The number of features in the hidden state `h`\n",
        "        batch_first: If ``True``, then the input and output tensors are provided\n",
        "            as `(batch, seq, feature)`\n",
        "\n",
        "    Inputs: input, h_0\n",
        "        - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features\n",
        "          of the input sequence. The input can also be a packed variable length\n",
        "          sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`\n",
        "          or :func:`torch.nn.utils.rnn.pack_sequence`\n",
        "          for details.\n",
        "        - **h_0** of shape `(num_directions, batch, hidden_size)`: tensor\n",
        "          containing the initial hidden state for each element in the batch.\n",
        "          Defaults to zero if not provided.\n",
        "\n",
        "    Outputs: output, h_n\n",
        "        - **output** of shape `(seq_len, batch, hidden_size * num_directions)`: tensor\n",
        "          containing the output features (`h_k`) from the last layer of the RNN,\n",
        "          for each `k`.  If a :class:`torch.nn.utils.rnn.PackedSequence` has\n",
        "          been given as the input, the output will also be a packed sequence.\n",
        "        - **h_n** (num_directions, batch, hidden_size): tensor\n",
        "          containing the hidden state for `k = seq_len`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, \n",
        "                 batch_first=False, bidirectional=False, recurrent_inits=None,\n",
        "                 **kwargs):\n",
        "        super(IndRNN_onlyrecurrent, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_first = batch_first\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        num_directions = 2 if self.bidirectional else 1\n",
        "\n",
        "        if batch_first:\n",
        "            self.time_index = 1\n",
        "            self.batch_index = 0\n",
        "        else:\n",
        "            self.time_index = 0\n",
        "            self.batch_index = 1\n",
        "\n",
        "        cells = []\n",
        "        directions = []\n",
        "        if recurrent_inits is not None:\n",
        "            kwargs[\"recurrent_init\"] = recurrent_inits\n",
        "        for dir in range(num_directions):\n",
        "            directions.append(IndRNNCell_onlyrecurrent(hidden_size, **kwargs))\n",
        "        self.cells = nn.ModuleList(directions)\n",
        "\n",
        "        h0 = torch.zeros(hidden_size * num_directions)\n",
        "        self.register_buffer('h0', torch.autograd.Variable(h0))\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        time_index = self.time_index\n",
        "        batch_index = self.batch_index\n",
        "        num_directions = 2 if self.bidirectional else 1\n",
        "        hidden_init = self.h0.unsqueeze(0).expand(\n",
        "            x.size(batch_index),\n",
        "            self.hidden_size * num_directions).contiguous()\n",
        "\n",
        "        x_n = []\n",
        "        for dir, cell in enumerate(self.cells):\n",
        "            hx_cell = hidden_init[\n",
        "                :, self.hidden_size * dir: self.hidden_size * (dir + 1)]\n",
        "            outputs = []\n",
        "            hiddens = []\n",
        "            x_T = torch.unbind(x, time_index)\n",
        "            if dir == 1:\n",
        "                x_T = reversed(x_T)\n",
        "            for x_t in x_T:\n",
        "                hx_cell = cell(x_t, hx_cell)\n",
        "                outputs.append(hx_cell)\n",
        "            if dir == 1:\n",
        "                outputs = outputs[::-1]\n",
        "            x_cell = torch.stack(outputs, time_index)\n",
        "            x_n.append(x_cell)\n",
        "            hiddens.append(hx_cell)\n",
        "        x = torch.cat(x_n, -1)\n",
        "        return x.squeeze(2), torch.cat(hiddens, -1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZZ_zpZj0vDGg",
        "colab_type": "code",
        "outputId": "340e3d7e-a764-4607-c75c-64c841803718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This code is to implement the IndRNN (only the recurrent part) using CUDA for fast computation. The CUDA part is similar the SRU implementation from \n",
        "https://github.com/taolei87/sru.\n",
        "This runs around 32 times faster than the general pytorch implementation on pixel MNIST example (sequence lengeth 784). For longer sequence, \n",
        "it will be even more efficient, and vice versa. \n",
        "Since this only contains the recurrent part of IndRNN, fully connected layers or convolutional layers are needed before it.\n",
        "Please cite the following paper if you find it useful.\n",
        "Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. \"Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN,\" \n",
        "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5457-5466. 2018.\n",
        "@inproceedings{li2018independently,\n",
        "  title={Independently recurrent neural network (indrnn): Building A longer and deeper RNN},\n",
        "  author={Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},\n",
        "  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n",
        "  pages={5457--5466},\n",
        "  year={2018}\n",
        "}\n",
        "\"\"\"\n",
        "!pip install cupy\n",
        "!pip install pynvrtc\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "#import warnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "\n",
        "from torch.nn import Parameter\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "from cupy.cuda import function\n",
        "from pynvrtc.compiler import Program\n",
        "from collections import namedtuple\n",
        "\n",
        "IndRNN_CODE = \"\"\"\n",
        "extern \"C\" {\n",
        "\n",
        "    __forceinline__ __device__ float reluf(float x)\n",
        "    {\n",
        "        return (x > 0.f) ? x : 0.f;\n",
        "    }\n",
        "\n",
        "    __forceinline__ __device__ float calc_grad_activation(float x)\n",
        "    {\n",
        "        return (x > 0.f) ? 1.f : 0.f;\n",
        "    }\n",
        "\n",
        "    __global__ void indrnn_fwd( const float * __restrict__ x,\n",
        "                            const float * __restrict__ weight_hh, const float * __restrict__ h0,\n",
        "                            const int len, const int batch, const int hidden_size, \n",
        "                            float * __restrict__ h)\n",
        "    {\n",
        "        int ncols = batch*hidden_size;\n",
        "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "        if (col >= ncols) return;       \n",
        "        const float weight_hh_cur = *(weight_hh + (col%hidden_size));\n",
        "        float cur = *(h0 + col);\n",
        "        const float *xp = x+col;\n",
        "        float *hp = h+col;\n",
        "\n",
        "        for (int row = 0; row < len; ++row)\n",
        "        {\n",
        "            cur=reluf(cur*weight_hh_cur+(*xp));\n",
        "            *hp=cur;\n",
        "            xp += ncols;\n",
        "            hp += ncols;            \n",
        "        }\n",
        "    }\n",
        "\n",
        "    __global__ void indrnn_bwd(const float * __restrict__ x,\n",
        "                             const float * __restrict__ weight_hh, const float * __restrict__ h0,\n",
        "                             const float * __restrict__ h,\n",
        "                            const float * __restrict__ grad_h, \n",
        "                            const int len, const int batch, const int hidden_size, \n",
        "                            float * __restrict__ grad_x,\n",
        "                            float * __restrict__ grad_weight_hh, float * __restrict__ grad_h0)\n",
        "    {    \n",
        "        int ncols = batch*hidden_size;\n",
        "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "        if (col >= ncols) return;        \n",
        "        const float weight_hh_cur = *(weight_hh + (col%hidden_size));\n",
        "        float gweight_hh_cur = 0;\n",
        "        float cur = 0;  // *(grad_last + col);        //0; strange gradient behavior. grad_last and grad_h, one of them is zero.     \n",
        "        \n",
        "        const float *xp = x+col + (len-1)*ncols;\n",
        "        const float *hp = h+col + (len-1)*ncols;      \n",
        "        float *gxp = grad_x + col + (len-1)*ncols;\n",
        "        const float *ghp = grad_h + col + (len-1)*ncols;\n",
        "        \n",
        "\n",
        "        for (int row = len-1; row >= 0; --row)\n",
        "        {        \n",
        "            const float prev_h_val = (row>0) ? (*(hp-ncols)) : (*(h0+col));\n",
        "            //float h_val_beforeact = prev_h_val*weight_hh_cur+(*xp);\n",
        "            float gh_beforeact = ((*ghp) + cur)*calc_grad_activation(prev_h_val*weight_hh_cur+(*xp));\n",
        "            cur = gh_beforeact*weight_hh_cur;\n",
        "            gweight_hh_cur += gh_beforeact*prev_h_val;\n",
        "            *gxp = gh_beforeact;\n",
        "\n",
        "            xp -= ncols;\n",
        "            hp -= ncols;\n",
        "            gxp -= ncols;\n",
        "            ghp -= ncols;        \n",
        "        }\n",
        "\n",
        "        atomicAdd(grad_weight_hh + (col%hidden_size), gweight_hh_cur);\n",
        "        *(grad_h0 +col) = cur;\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class IndRNN_Compute_GPU(Function):\n",
        "\n",
        "    _IndRNN_PROG = Program(IndRNN_CODE, 'indrnn_prog.cu')#.encode('utf-8')  .encode()\n",
        "    _IndRNN_PTX = _IndRNN_PROG.compile()\n",
        "    _DEVICE2FUNC = {}\n",
        "\n",
        "    def __init__(self,gradclipvalue=0):\n",
        "        super(IndRNN_Compute_GPU, self).__init__()\n",
        "        self.gradclipvalue=gradclipvalue\n",
        "\n",
        "    def compile_functions(self):\n",
        "        device = torch.cuda.current_device()\n",
        "        print ('IndRNN loaded for gpu {}'.format(device))\n",
        "        mod = function.Module()\n",
        "        mod.load(bytes(self._IndRNN_PTX.encode()))\n",
        "        fwd_func = mod.get_function('indrnn_fwd')\n",
        "        bwd_func = mod.get_function('indrnn_bwd')\n",
        "\n",
        "        Stream = namedtuple('Stream', ['ptr'])\n",
        "        current_stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n",
        "\n",
        "        self._DEVICE2FUNC[device] = (current_stream, fwd_func, bwd_func)\n",
        "        return current_stream, fwd_func, bwd_func\n",
        "\n",
        "    def get_functions(self):\n",
        "        res = self._DEVICE2FUNC.get(torch.cuda.current_device(), None)\n",
        "        return res if res else self.compile_functions()\n",
        "\n",
        "    def forward(self, x, weight_hh, h0):\n",
        "        length = x.size(0) if x.dim() == 3 else 1\n",
        "        batch = x.size(-2)\n",
        "        hidden_size = x.size(-1)  #hidden_size\n",
        "        ncols = batch*hidden_size\n",
        "        thread_per_block = min(512, ncols)\n",
        "        num_block = (ncols-1)//thread_per_block+1\n",
        "        \n",
        "        size = (length, batch, hidden_size) if x.dim() == 3 else (batch, hidden_size)\n",
        "        h = x.new(*size)\n",
        "\n",
        "        stream, fwd_func, _ = self.get_functions()\n",
        "        FUNC = fwd_func\n",
        "        FUNC(args=[\n",
        "            x.contiguous().data_ptr(),\n",
        "            weight_hh.contiguous().data_ptr(),\n",
        "            h0.contiguous().data_ptr(),\n",
        "            length,\n",
        "            batch,\n",
        "            hidden_size,\n",
        "            h.contiguous().data_ptr()],\n",
        "            block = (thread_per_block,1,1), grid = (num_block,1,1),\n",
        "            stream=stream\n",
        "        )\n",
        "\n",
        "        self.save_for_backward(x, h, weight_hh, h0)#\n",
        "        return h\n",
        "\n",
        "    def backward(self, grad_h):\n",
        "        x, h, weight_hh, h0 = self.saved_tensors\n",
        "        length = x.size(0) if x.dim() == 3 else 1\n",
        "        batch = x.size(-2)\n",
        "        hidden_size = x.size(-1)#self.hidden_size\n",
        "        ncols = batch*hidden_size\n",
        "        thread_per_block = min(512, ncols)\n",
        "        num_block = (ncols-1)//thread_per_block+1\n",
        "\n",
        "        grad_x = x.new(*x.size())\n",
        "        grad_weight_hh = x.new(hidden_size).zero_()\n",
        "        grad_h0 = x.new(batch, hidden_size)  \n",
        "\n",
        "        stream, _, bwd_func = self.get_functions()\n",
        "        FUNC = bwd_func\n",
        "        FUNC(args=[\n",
        "            x.contiguous().data_ptr(),\n",
        "            weight_hh.contiguous().data_ptr(),\n",
        "            h0.contiguous().data_ptr(),\n",
        "            h.contiguous().data_ptr(),\n",
        "            grad_h.contiguous().data_ptr(),\n",
        "            length,\n",
        "            batch,\n",
        "            hidden_size,\n",
        "            grad_x.contiguous().data_ptr(),\n",
        "            grad_weight_hh.contiguous().data_ptr(),\n",
        "            grad_h0.contiguous().data_ptr()],\n",
        "            block = (thread_per_block,1,1), grid = (num_block,1,1),\n",
        "            stream=stream\n",
        "        )\n",
        "        if self.gradclipvalue>0:\n",
        "            grad_x.clamp_(-self.gradclipvalue,self.gradclipvalue)\n",
        "            grad_weight_hh.clamp_(-self.gradclipvalue,self.gradclipvalue)\n",
        "            grad_h0.clamp_(-self.gradclipvalue,self.gradclipvalue)\n",
        "        return grad_x, grad_weight_hh, grad_h0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class cuda_IndRNN_onlyrecurrent(nn.Module):\n",
        "    def __init__(self, hidden_size, gradclipvalue=0,\n",
        "                 hidden_max_abs=None, recurrent_init=None):\n",
        "        super(cuda_IndRNN_onlyrecurrent, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.recurrent_init = recurrent_init\n",
        "        self.weight_hh = Parameter(torch.Tensor(hidden_size))   \n",
        "        self.gradclipvalue=gradclipvalue         \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for name, weight in self.named_parameters():\n",
        "            if \"weight_hh\" in name:\n",
        "                if self.recurrent_init is None:\n",
        "                    nn.init.uniform(weight, a=0, b=1)\n",
        "                else:\n",
        "                    self.recurrent_init(weight)\n",
        "\n",
        "    def forward(self, input, h0=None):\n",
        "        assert input.dim() == 2 or input.dim() == 3        \n",
        "        if h0 is None:\n",
        "            h0 = input.data.new(input.size(-2),input.size(-1)).zero_()\n",
        "        IndRNN_Compute = IndRNN_Compute_GPU(self.gradclipvalue)\n",
        "        #h=IndRNN_Compute(input, self.weight_hh, h0)\n",
        "        return IndRNN_Compute(input, self.weight_hh, h0)\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cupy in /usr/local/lib/python2.7/dist-packages (5.4.0)\n",
            "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python2.7/dist-packages (from cupy) (0.4)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from cupy) (1.16.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from cupy) (1.11.0)\n",
            "Requirement already satisfied: pynvrtc in /usr/local/lib/python2.7/dist-packages (9.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Uz-WO0vu6OI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "import torch.nn.init as weight_init\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "#from cuda_IndRNN_onlyrecurrent import IndRNN_onlyrecurrent as IndRNN\n",
        "#from IndRNN_onlyrecurrent import IndRNN_onlyrecurrent as IndRNN \n",
        "class Batch_norm_step(nn.Module):\n",
        "    def __init__(self,  hidden_size,seq_len):\n",
        "        super(Batch_norm_step, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.max_time_step=seq_len\n",
        "        self.bn = nn.BatchNorm1d(hidden_size) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x=x.permute(1,2,0)\n",
        "        x= self.bn(x.clone())\n",
        "        x=x.permute(2,0,1)\n",
        "        return x\n",
        "class Dropout_overtime(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, input, p=0.5,training=False):\n",
        "    output = input.clone()\n",
        "    noise = input.data.new(input.size(-2),input.size(-1))  #torch.ones_like(input[0])\n",
        "    if training:            \n",
        "      noise.bernoulli_(1 - p).div_(1 - p)\n",
        "      noise = noise.unsqueeze(0).expand_as(input)\n",
        "      output.mul_(noise)\n",
        "    ctx.save_for_backward(noise)\n",
        "    ctx.training=training\n",
        "    return output\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    noise,=ctx.saved_tensors\n",
        "    if ctx.training:\n",
        "      return grad_output.mul(noise),None,None\n",
        "    else:\n",
        "      return grad_output,None,None\n",
        "dropout_overtime=Dropout_overtime.apply\n",
        "\n",
        "#import argparse\n",
        "#import opts     \n",
        "parser = argparse.ArgumentParser(description='pytorch action')\n",
        "#train_opts(parser)\n",
        "#args = parser.parse_args()\n",
        "#MAG=args.MAG\n",
        "U_bound=np.power(10,(np.log10(MAG)/seq_len))\n",
        "U_lowbound=np.power(10,(np.log10(1.0/MAG)/seq_len))  \n",
        "  \n",
        "class stackedIndRNN_encoder(nn.Module):\n",
        "    def __init__(self, input_size, outputclass):\n",
        "        super(stackedIndRNN_encoder, self).__init__()        \n",
        "        #hidden_size=args.hidden_size\n",
        "        \n",
        "        self.DIs=nn.ModuleList()\n",
        "        denseinput=nn.Linear(input_size*n_dimension, hidden_size, bias=True)\n",
        "        self.DIs.append(denseinput)\n",
        "        for x in range(num_layers - 1):\n",
        "            denseinput = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "            self.DIs.append(denseinput)                \n",
        "        \n",
        "        self.BNs = nn.ModuleList()\n",
        "        for x in range(num_layers):\n",
        "            bn = Batch_norm_step(hidden_size,seq_len)\n",
        "            self.BNs.append(bn)                      \n",
        "  \n",
        "        self.RNNs = nn.ModuleList()\n",
        "        rnn = cuda_IndRNN_onlyrecurrent(hidden_size=hidden_size) #IndRNN\n",
        "        self.RNNs.append(rnn)  \n",
        "        for x in range(num_layers-1):\n",
        "            rnn = cuda_IndRNN_onlyrecurrent(hidden_size=hidden_size) #IndRNN\n",
        "            self.RNNs.append(rnn)         \n",
        "            \n",
        "        self.lastfc = nn.Linear(hidden_size, outputclass, bias=True)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "      for name, param in self.named_parameters():\n",
        "        if 'weight_hh' in name:\n",
        "          param.data.uniform_(0,U_bound)          \n",
        "        if 'RNNs.'+str(num_layers-1)+'.weight_hh' in name:\n",
        "          param.data.uniform_(U_lowbound,U_bound)    \n",
        "        if 'DIs' in name and 'weight' in name:\n",
        "          param.data.uniform_(-ini_in2hid,ini_in2hid)               \n",
        "        if 'bns' in name and 'weight' in name:\n",
        "          param.data.fill_(1)      \n",
        "        if 'bias' in name:\n",
        "          param.data.fill_(0.0)              \n",
        "    def forward(self, input):\n",
        "        all_output = []\n",
        "        rnnoutputs={}\n",
        "        hidden_x={}               \n",
        "        seq_len, batch_size, indim,_=input.size()\n",
        "             \n",
        "        input=input.view(seq_len,batch_size,n_dimension*indim)                  \n",
        "        for x in range(1,len(self.RNNs)+1):\n",
        "          hidden_x['hidden%d'%x]=Variable(torch.zeros(1,batch_size,hidden_size).cuda())\n",
        "                            \n",
        "        rnnoutputs['rnnlayer0']=input\n",
        "        for x in range(1,len(self.RNNs)+1):\n",
        "          rnnoutputs['rnnlayer%d'%(x-1)]=rnnoutputs['rnnlayer%d'%(x-1)].view(seq_len*batch_size,-1)\n",
        "          rnnoutputs['rnnlayer%d'%(x-1)]=self.DIs[x-1](rnnoutputs['rnnlayer%d'%(x-1)])   \n",
        "          rnnoutputs['rnnlayer%d'%(x-1)]=rnnoutputs['rnnlayer%d'%(x-1)].view(seq_len,batch_size,-1)  \n",
        "          rnnoutputs['rnnlayer%d'%x]= self.RNNs[x-1](rnnoutputs['rnnlayer%d'%(x-1)], hidden_x['hidden%d'%x])        \n",
        "          rnnoutputs['rnnlayer%d'%x]=self.BNs[x-1](rnnoutputs['rnnlayer%d'%x])     \n",
        "          if dropout>0:\n",
        "            rnnoutputs['rnnlayer%d'%x]= dropout_overtime(rnnoutputs['rnnlayer%d'%x],dropout,self.training) \n",
        "        temp=rnnoutputs['rnnlayer%d'%len(self.RNNs)][-1]\n",
        "        output = self.lastfc(temp)\n",
        "        return output                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QC-4r4R3pPRh",
        "colab_type": "code",
        "outputId": "50c98336-1631-4874-e7b5-bcbfa9a71eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "fluff, train_id = train_link.split('=')\n",
        "print (train_id) # Verify that you have everything after '='\n",
        "\n",
        "fluff, train_len_id = train_len_link.split('=')\n",
        "print (train_len_id) # Verify that you have everything after '='\n",
        "\n",
        "fluff, train_label_id = train_label_link.split('=')\n",
        "print (train_label_id) # Verify that you have everything after '='\n",
        "\n",
        "fluff, test_id = test_link.split('=')\n",
        "print (test_id) # Verify that you have everything after '='\n",
        "\n",
        "fluff, test_len_id = test_len_link.split('=')\n",
        "print (test_len_id) # Verify that you have everything after '='\n",
        "\n",
        "fluff, test_label_id = test_label_link.split('=')\n",
        "print (test_label_id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':train_id}) \n",
        "downloaded.GetContentFile('train.npy')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':train_len_id}) \n",
        "downloaded.GetContentFile('train_len.npy')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':train_label_id}) \n",
        "downloaded.GetContentFile('train_label.npy')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':test_id}) \n",
        "downloaded.GetContentFile('test.npy') \n",
        "\n",
        "downloaded = drive.CreateFile({'id':test_len_id}) \n",
        "downloaded.GetContentFile('test_len.npy') \n",
        "\n",
        "downloaded = drive.CreateFile({'id':test_label_id}) \n",
        "downloaded.GetContentFile('test_label.npy') \n",
        "\n",
        "train_datasets='train'\n",
        "test_dataset='test'"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "199wLS6SYhLwAXumTY483ACoqsaQ_ADmP\n",
            "1alMuIeD-1wjAlsJFPfAm_7RiNCJ4-KY-\n",
            "1BILzI1hUwtMEd0RUia6MCr3towN5TxU3\n",
            "199wLS6SYhLwAXumTY483ACoqsaQ_ADmP\n",
            "1alMuIeD-1wjAlsJFPfAm_7RiNCJ4-KY-\n",
            "1BILzI1hUwtMEd0RUia6MCr3towN5TxU3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xMuT59gfnbui",
        "colab_type": "code",
        "outputId": "fb4e5e52-8e2d-4859-9546-c7fdd4e0423e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import h5py\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "#import glob\n",
        "#import skimage.transform\n",
        "#from skimage import color\n",
        "import pickle\n",
        "#import theano\n",
        "#import cv2\n",
        "from multiprocessing import Pool\n",
        "from threading import Thread\n",
        "import os.path\n",
        "#RGB_frames = '/home/sl669/caffe/colordataset/ImageNET/ILSVRC2015/Data/CLS-LOC/val/'#'/home/sl669/caffe/ucf101/framearrays/'#\n",
        "\n",
        "#from __main__ import train_datasets\n",
        "#train_datasets='train_ntus'\n",
        "datasets=train_datasets\n",
        "dataname=datasets+'.npy'\n",
        "labelname=datasets+'_label.npy'\n",
        "lenname=datasets+'_len.npy'\n",
        "data_handle=np.load(dataname)\n",
        "label_handle=np.load(labelname)\n",
        "len_handle=np.load(lenname)\n",
        "num_videos = len(data_handle)  \n",
        "train_no=int(num_videos*0.95)\n",
        "test_no=num_videos-train_no\n",
        "\n",
        "shufflevideolist=np.arange(num_videos)\n",
        "np.random.shuffle(shufflevideolist)\n",
        "\n",
        "shufflevideolist_train=shufflevideolist[:train_no]\n",
        "shufflevideolist_test=shufflevideolist[train_no:]\n",
        "\n",
        "print ('Dataset train size, test size', train_no,test_no)\n",
        "\n",
        "\n",
        "def rotate( input,s,b):\n",
        "  shape=input.shape\n",
        "  input=input.reshape((-1,3))\n",
        "  XT=input[:,0]\n",
        "  YT=input[:,1]\n",
        "  ZT=input[:,2]\n",
        "  s=s/180.0*np.pi\n",
        "  b=b/180.0*np.pi\n",
        "  RX = XT*np.cos(b) - ZT*np.sin(b) + ZT*np.sin(b)*np.cos(s) + YT*np.sin(b)*np.sin(s) - ZT*np.sin(b)*(np.cos(s) - 1);\n",
        "  RY = YT*np.cos(s);\n",
        "  RZ = ZT*np.cos(b)*np.cos(s) - ZT*(np.cos(b) - 1) - XT*np.sin(b) + YT*np.cos(b)*np.sin(s) - ZT*np.cos(b)*(np.cos(s) - 1);\n",
        "  RX=RX.reshape((-1,1))\n",
        "  RY=RY.reshape((-1,1))\n",
        "  RZ=RZ.reshape((-1,1))\n",
        "  output=np.concatenate([RX,RY,RZ],axis=1)\n",
        "  output=output.reshape(shape)\n",
        "  #print(shape,output.shape,input.shape)\n",
        "  return output \n",
        "\n",
        "class batch_thread_train():\n",
        "  def __init__(self, result, batch_size_,seq_len,use_rotation=False):\n",
        "    self.result = result\n",
        "    self.batch_size_=batch_size_\n",
        "    self.seq_len=seq_len\n",
        "    self.idx=0    \n",
        "    self.use_rotation=use_rotation\n",
        "  \n",
        "  def __call__(self):###Be careful.  The appended data may change like pointer.\n",
        "    templabel=[] \n",
        "    batch_data=[]\n",
        "    for j in range(self.batch_size_):\n",
        "      self.idx +=1\n",
        "      if self.idx == train_no:\n",
        "        self.idx =0\n",
        "        np.random.shuffle(shufflevideolist_train)\n",
        "      shufflevideoindex=shufflevideolist_train[self.idx]\n",
        "      \n",
        "      \n",
        "      label=label_handle[shufflevideoindex]     \n",
        "      templabel.append(np.int32(label))  \n",
        "      dataset=data_handle[shufflevideoindex]\n",
        "      len_data=len_handle[shufflevideoindex] \n",
        "      \n",
        "      #dataset = np.array(dataset)\n",
        "      \n",
        "      sample=np.zeros(tuple((self.seq_len,)+dataset.shape[1:]))\n",
        "      lenperseg=len_data//self.seq_len\n",
        "      if lenperseg==1 and len_data>self.seq_len:\n",
        "        startid=np.random.randint(len_data-self.seq_len)\n",
        "        sample=dataset[startid:startid+self.seq_len]\n",
        "        #print('wrong data length first')\n",
        "      elif len_data<=self.seq_len:\n",
        "        startid=np.random.randint(max(self.seq_len-len_data,int(0.25*self.seq_len)))\n",
        "        endid=min(self.seq_len,startid+len_data)\n",
        "        datasid=0\n",
        "        dataeid=len_data\n",
        "        if startid+len_data>self.seq_len:\n",
        "          datasid=np.random.randint(startid+len_data-self.seq_len)\n",
        "          dataeid=datasid+self.seq_len-startid\n",
        "        sample[startid:endid]=dataset[datasid:dataeid]\n",
        "      else:      \n",
        "        for framei in range(self.seq_len):        \n",
        "          if framei==self.seq_len-1:\n",
        "            index=lenperseg*framei + np.random.randint(len_data-lenperseg*(self.seq_len-1))\n",
        "          else:\n",
        "            index=lenperseg*framei + np.random.randint(lenperseg)    \n",
        "          sample[framei]=dataset[index]\n",
        "          \n",
        "      \n",
        "      if self.use_rotation:\n",
        "        if np.random.randint(2):\n",
        "          s=np.random.randint(2)*45#random(1)*45\n",
        "          b=np.random.randint(2)*45#random(1)*45\n",
        "          #print(sample.shape)\n",
        "          sample=rotate(sample,s,b)\n",
        "        #print (index,lenperseg)  \n",
        "#       rframei=np.random.randint(len_data)  \n",
        "#       tmean=(dataset[rframei,0,:]+dataset[rframei,12,:]+dataset[rframei,16,:])/3\n",
        "#       sample=sample-tmean  \n",
        "      batch_data.append(sample) ###Be careful. It has to be different. Otherwise, the appended data will change as well.\n",
        "      #print(batch_data)  \n",
        "    \n",
        "      \n",
        "    self.result['data']=np.asarray(batch_data,dtype=np.float32)\n",
        "    self.result['label']= np.asarray(templabel,dtype=np.int32)\n",
        "    \n",
        "\n",
        "class DataHandler_train(object):\n",
        "\n",
        "  def __init__(self, batch_size, seq_len, use_rotation=False):#datasets,\n",
        "    self.batch_size_ = batch_size\t\t\n",
        "    #self.datasets = datasets    \n",
        "    random.seed(10)  \n",
        "    \n",
        "    self.thread_result = {}\n",
        "    self.thread = None\n",
        "\n",
        "    self.batch_advancer =batch_thread_train(self.thread_result,self.batch_size_,seq_len,use_rotation)\n",
        "    \n",
        "    self.dispatch_worker()\n",
        "    self.join_worker()\n",
        "\n",
        "\n",
        "  def GetBatch(self):\n",
        "    #self.batch_data_  = np.zeros((self.batch_size_, 3, self.seq_length_, 112, 112), dtype=np.float32)\n",
        "    if self.thread is not None:\n",
        "      self.join_worker() \n",
        "\n",
        "    self.batch_data_=self.thread_result['data']\n",
        "    self.batch_label_= self.thread_result['label']\n",
        "        \n",
        "    self.dispatch_worker()\n",
        "    return self.batch_data_, self.batch_label_\n",
        "\n",
        "  def dispatch_worker(self):\n",
        "    assert self.thread is None\n",
        "    self.thread = Thread(target=self.batch_advancer)\n",
        "    self.thread.start()\n",
        "\n",
        "  def join_worker(self):\n",
        "    assert self.thread is not None\n",
        "    self.thread.join()\n",
        "    self.thread = None\n",
        "    \n",
        "  def GetDatasetSize(self):\n",
        "    return train_no\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class batch_thread_eval():\n",
        "  def __init__(self, result, batch_size_,seq_len):\n",
        "    self.result = result\n",
        "    self.batch_size_=batch_size_\n",
        "    self.seq_len=seq_len\n",
        "    self.idx=0    \n",
        "  \n",
        "  def __call__(self):###Be careful.  The appended data may change like pointer.\n",
        "    templabel=[] \n",
        "    batch_data=[]\n",
        "    for j in range(self.batch_size_):\n",
        "      self.idx +=1\n",
        "      if self.idx == test_no:\n",
        "        self.idx =0\n",
        "        np.random.shuffle(shufflevideolist_test)\n",
        "      shufflevideoindex=shufflevideolist_test[self.idx]\n",
        "      \n",
        "      \n",
        "      label=label_handle[shufflevideoindex]     \n",
        "      templabel.append(np.int32(label))  \n",
        "      dataset=data_handle[shufflevideoindex]\n",
        "      len_data=len_handle[shufflevideoindex]\n",
        "      \n",
        "      #dataset = np.array(dataset)\n",
        "      \n",
        "      sample=np.zeros(tuple((self.seq_len,)+dataset.shape[1:]))\n",
        "      lenperseg=len_data//self.seq_len\n",
        "      if lenperseg==1 and len_data>self.seq_len:\n",
        "        startid=np.random.randint(len_data-self.seq_len)\n",
        "        sample=dataset[startid:startid+self.seq_len]\n",
        "      elif len_data<=self.seq_len:\n",
        "        startid=np.random.randint(max(self.seq_len-len_data,int(0.25*self.seq_len)))\n",
        "        endid=min(self.seq_len,startid+len_data)\n",
        "        datasid=0\n",
        "        dataeid=len_data\n",
        "        if startid+len_data>self.seq_len:\n",
        "          datasid=np.random.randint(startid+len_data-self.seq_len)\n",
        "          dataeid=datasid+self.seq_len-startid\n",
        "        sample[startid:endid]=dataset[datasid:dataeid]\n",
        "      else:      \n",
        "        for framei in range(self.seq_len):        \n",
        "          if framei==self.seq_len-1:\n",
        "            index=lenperseg*framei + np.random.randint(len_data-lenperseg*(self.seq_len-1))\n",
        "          else:\n",
        "            index=lenperseg*framei + np.random.randint(lenperseg)    \n",
        "          sample[framei]=dataset[index]\n",
        "        #print (index,lenperseg)  \n",
        "        \n",
        "      batch_data.append(sample) ###Be careful. It has to be different. Otherwise, the appended data will change as well.\n",
        "      #print(batch_data)       \n",
        "      \n",
        "    self.result['data']=np.asarray(batch_data,dtype=np.float32)\n",
        "    self.result['label']= np.asarray(templabel,dtype=np.int32)   \n",
        "\n",
        "class DataHandler_eval(object):\n",
        "\n",
        "  def __init__(self, batch_size, seq_len):#, datasets\n",
        "    self.batch_size_ = batch_size    \n",
        "    #self.datasets = datasets    \n",
        "    random.seed(10)  \n",
        "    \n",
        "    self.thread_result = {}\n",
        "    self.thread = None\n",
        "\n",
        "    self.batch_advancer =batch_thread_eval(self.thread_result,self.batch_size_,seq_len)\n",
        "    \n",
        "    self.dispatch_worker()\n",
        "    self.join_worker()\n",
        "\n",
        "\n",
        "  def GetBatch(self):\n",
        "    #self.batch_data_  = np.zeros((self.batch_size_, 3, self.seq_length_, 112, 112), dtype=np.float32)\n",
        "    if self.thread is not None:\n",
        "      self.join_worker() \n",
        "\n",
        "    self.batch_data_=self.thread_result['data']\n",
        "    self.batch_label_= self.thread_result['label']\n",
        "        \n",
        "    self.dispatch_worker()\n",
        "    return self.batch_data_, self.batch_label_\n",
        "\n",
        "  def dispatch_worker(self):\n",
        "    assert self.thread is None\n",
        "    self.thread = Thread(target=self.batch_advancer)\n",
        "    self.thread.start()\n",
        "\n",
        "  def join_worker(self):\n",
        "    assert self.thread is not None\n",
        "    self.thread.join()\n",
        "    self.thread = None\n",
        "    \n",
        "  def GetDatasetSize(self):\n",
        "    return test_no\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset train size, test size 1900 101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lsPN48aPn-_4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import h5py\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "#import glob\n",
        "#import skimage.transform\n",
        "#from skimage import color\n",
        "import pickle\n",
        "#import theano\n",
        "#import cv2\n",
        "from multiprocessing import Pool\n",
        "from threading import Thread\n",
        "import os.path\n",
        "#RGB_frames = '/home/sl669/caffe/colordataset/ImageNET/ILSVRC2015/Data/CLS-LOC/val/'#'/home/sl669/caffe/ucf101/framearrays/'#\n",
        "\n",
        "\n",
        "\n",
        "#from __main__ import test_dataset\n",
        "datasets=test_dataset\n",
        "class batch_thread():\n",
        "  def __init__(self, result, batch_size_,seq_len):#, datasets\n",
        "    self.result = result\n",
        "    self.batch_size_=batch_size_\n",
        "    self.datasets = datasets   \n",
        "    self.seq_len=seq_len\n",
        "    self.idx=-1\n",
        "    \n",
        "    dataname=datasets+'.npy'\n",
        "    labelname=datasets+'_label.npy'\n",
        "    lenname=datasets+'_len.npy'\n",
        "    self.data_handle=np.load(dataname)\n",
        "    self.label_handle=np.load(labelname)\n",
        "    self.len_handle=np.load(lenname) \n",
        "    \n",
        "    self.num_videos = len(self.data_handle)    \n",
        "    self.shufflevideolist=np.arange(self.num_videos)\n",
        "    np.random.shuffle(self.shufflevideolist)\n",
        "\n",
        "    print ('Dataset size', self.num_videos)\n",
        "  \n",
        "  def __call__(self):###Be careful.  The appended data may change like pointer.\n",
        "    templabel=[] \n",
        "    batch_data=[]\n",
        "    tempindex=[] \n",
        "    for j in range(self.batch_size_):\n",
        "      self.idx +=1\n",
        "      if self.idx == self.num_videos:\n",
        "        self.idx =0\n",
        "        np.random.shuffle(self.shufflevideolist)\n",
        "      shufflevideoindex=self.shufflevideolist[self.idx]\n",
        "      \n",
        "      label=self.label_handle[shufflevideoindex]     \n",
        "      templabel.append(np.int32(label))  \n",
        "      tempindex.append(np.int32(shufflevideoindex)) \n",
        "      dataset=self.data_handle[shufflevideoindex]\n",
        "      len_data=self.len_handle[shufflevideoindex] \n",
        "      \n",
        "      #dataset = np.array(dataset)\n",
        "      \n",
        "      sample=np.zeros(tuple((self.seq_len,)+dataset.shape[1:]))\n",
        "      lenperseg=len_data//self.seq_len\n",
        "      if lenperseg==1 and len_data>self.seq_len:\n",
        "        startid=np.random.randint(len_data-self.seq_len)\n",
        "        sample=dataset[startid:startid+self.seq_len]\n",
        "      elif len_data<=self.seq_len:\n",
        "        startid=np.random.randint(max(self.seq_len-len_data,int(0.25*self.seq_len)))\n",
        "        endid=min(self.seq_len,startid+len_data)\n",
        "        datasid=0\n",
        "        dataeid=len_data\n",
        "        if startid+len_data>self.seq_len:\n",
        "          datasid=np.random.randint(startid+len_data-self.seq_len)\n",
        "          dataeid=datasid+self.seq_len-startid\n",
        "        sample[startid:endid]=dataset[datasid:dataeid]\n",
        "      else:      \n",
        "        for framei in range(self.seq_len):        \n",
        "          if framei==self.seq_len-1:\n",
        "            index=lenperseg*framei + np.random.randint(len_data-lenperseg*(self.seq_len-1))\n",
        "          else:\n",
        "            index=lenperseg*framei + np.random.randint(lenperseg)    \n",
        "          sample[framei]=dataset[index]\n",
        "        #print (index,lenperseg)  \n",
        "        \n",
        "      batch_data.append(sample) ###Be careful. It has to be different. Otherwise, the appended data will change as well.\n",
        "      #print(batch_data)       \n",
        "\n",
        "       \n",
        "    self.result['data']=np.asarray(batch_data,dtype=np.float32)\n",
        "    self.result['label']= np.asarray(templabel,dtype=np.int32)   \n",
        "    self.result['index']= np.asarray(tempindex,dtype=np.int32)   \n",
        "      \n",
        "      \n",
        "  def GetDatasetSize(self):\n",
        "    return self.num_videos\n",
        "\n",
        "\n",
        "\n",
        "class testDataHandler(object):\n",
        "\n",
        "  def __init__(self, batch_size, seq_len):#, datasets\n",
        "    self.batch_size_ = batch_size\t\t\n",
        "    #self.datasets = datasets    \n",
        "    random.seed(10)  \n",
        "    \n",
        "    self.thread_result = {}\n",
        "    self.thread = None\n",
        "\n",
        "    self.batch_advancer =batch_thread(self.thread_result,self.batch_size_,seq_len)#, self.datasets\n",
        "    \n",
        "    self.datasetsize=self.batch_advancer.GetDatasetSize()\n",
        "    \n",
        "    self.dispatch_worker()\n",
        "    self.join_worker()\n",
        "\n",
        "\n",
        "  def GetBatch(self):\n",
        "    #self.batch_data_  = np.zeros((self.batch_size_, 3, self.seq_length_, 112, 112), dtype=np.float32)\n",
        "    if self.thread is not None:\n",
        "      self.join_worker() \n",
        "      \n",
        "#     self.batch_data_=self.thread_result['data']\n",
        "#     self.batch_label_=self.thread_result['label']\n",
        "\n",
        "    self.batch_data_=self.thread_result['data']\n",
        "    self.batch_label_= self.thread_result['label']\n",
        "    self.batch_index_= self.thread_result['index']\n",
        "        \n",
        "    self.dispatch_worker()\n",
        "    return self.batch_data_, self.batch_label_,self.batch_index_\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  def dispatch_worker(self):\n",
        "    assert self.thread is None\n",
        "    self.thread = Thread(target=self.batch_advancer)\n",
        "    self.thread.start()\n",
        "\n",
        "  def join_worker(self):\n",
        "    assert self.thread is not None\n",
        "    self.thread.join()\n",
        "    self.thread = None\n",
        "    \n",
        "  def GetDatasetSize(self):\n",
        "    return self.datasetsize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X2vkp4NwvGM1",
        "colab_type": "code",
        "outputId": "098379c8-8ae6-4bbf-a7c1-56c6f93356ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 24610
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "#import argparse\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "seed=100\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  pass\n",
        "else:\n",
        "  print(\"WARNING: CUDA not available\")\n",
        "\n",
        "#import opts     \n",
        "#parser = argparse.ArgumentParser(description='pytorch action')\n",
        "#opts.train_opts(parser)\n",
        "#args = parser.parse_args()\n",
        "#print(args)\n",
        "\n",
        "#import Indrnn_action_network\n",
        "\n",
        "#batch_size = args.batch_size\n",
        "#seq_len=args.seq_len\n",
        "in_size=num_joints\n",
        "gradientclip_value=10\n",
        "#U_bound=Indrnn_action_network.U_bound\n",
        "\n",
        "\n",
        "\n",
        "model = stackedIndRNN_encoder(in_size, outputclass)  \n",
        "model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Adam with lr 2e-4 works fine.\n",
        "learning_rate=lr\n",
        "if use_weightdecay_nohiddenW:\n",
        "  param_decay=[]\n",
        "  param_nodecay=[]\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'weight_hh' in name or 'bias' in name:\n",
        "      param_nodecay.append(param)      \n",
        "      #print('parameters no weight decay: ',name)          \n",
        "    else:\n",
        "      param_decay.append(param)      \n",
        "      #print('parameters with weight decay: ',name)          \n",
        "\n",
        "  if opti=='sgd':\n",
        "    optimizer = torch.optim.SGD([\n",
        "            {'params': param_nodecay},\n",
        "            {'params': param_decay, 'weight_decay': decayfactor}\n",
        "        ], lr=learning_rate,momentum=0.9,nesterov=True)   \n",
        "  else:                \n",
        "    optimizer = torch.optim.Adam([\n",
        "            {'params': param_nodecay},\n",
        "            {'params': param_decay, 'weight_decay': decayfactor}\n",
        "        ], lr=learning_rate) \n",
        "else:  \n",
        "  if opti=='sgd':   \n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate,momentum=0.9,nesterov=True)\n",
        "  else:                      \n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  \n",
        "#from data_reader_numpy_witheval import DataHandler_train,DataHandler_eval  \n",
        "#from data_reader_numpy_test import DataHandler as testDataHandler\n",
        "dh_train = DataHandler_train(batch_size,seq_len)\n",
        "dh_eval = DataHandler_eval(batch_size,seq_len)\n",
        "dh_test= testDataHandler(batch_size,seq_len)\n",
        "num_train_batches=int(np.ceil(dh_train.GetDatasetSize()/(batch_size+0.0)))\n",
        "num_eval_batches=int(np.ceil(dh_eval.GetDatasetSize()/(batch_size+0.0)))\n",
        "num_test_batches=int(np.ceil(dh_test.GetDatasetSize()/(batch_size+0.0)))\n",
        "#print(num_train_batches)\n",
        "\n",
        "\n",
        "def train(num_train_batches):\n",
        "  model.train()\n",
        "  tacc=0\n",
        "  count=0\n",
        "  start_time = time.time()\n",
        "  for batchi in range(0,num_train_batches):\n",
        "    inputs,targets=dh_train.GetBatch()\n",
        "    inputs=inputs.transpose(1,0,2,3)\n",
        "    #print(inputs.shape)\n",
        "    \n",
        "    inputs=Variable(torch.from_numpy(inputs).cuda(), requires_grad=True)\n",
        "    targets=Variable(torch.from_numpy(np.int64(targets)).cuda(), requires_grad=False)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if constrain_U:\n",
        "      clip_weight(model,U_bound)\n",
        "    output=model(inputs)\n",
        "    loss = criterion(output, targets)\n",
        "\n",
        "    pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "    accuracy = pred.eq(targets.data).cpu().sum().numpy()/(0.0+targets.size(0))      \n",
        "          \n",
        "    loss.backward()\n",
        "    clip_gradient(model,gradientclip_value)\n",
        "    optimizer.step()\n",
        "    \n",
        "    tacc=tacc+accuracy#loss.data.cpu().numpy()#accuracy\n",
        "    count+=1\n",
        "  elapsed = time.time() - start_time\n",
        "  print (\"training accuracy: \", tacc/(count+0.0)  )\n",
        "  #print ('time per batch: ', elapsed/num_train_batches)\n",
        "  #print ('time per epoch: ', elapsed)\n",
        "  \n",
        "def set_bn_train(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('BatchNorm') != -1:\n",
        "      m.train()       \n",
        "def eval(dh,num_batches,use_bn_trainstat=False):\n",
        "  model.eval()\n",
        "  if use_bn_trainstat:\n",
        "    model.apply(set_bn_train)\n",
        "  tacc=0\n",
        "  count=0  \n",
        "  start_time = time.time()\n",
        "  while(1):  \n",
        "    inputs,targets=dh.GetBatch()\n",
        "    inputs=inputs.transpose(1,0,2,3)\n",
        "    inputs=Variable(torch.from_numpy(inputs).cuda())\n",
        "    targets=Variable(torch.from_numpy(np.int64(targets)).cuda())\n",
        "        \n",
        "    output=model(inputs)\n",
        "    pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "    accuracy = pred.eq(targets.data).cpu().sum().numpy()        \n",
        "    tacc+=accuracy\n",
        "    count+=1\n",
        "    if count==num_batches*eval_fold:\n",
        "      break\n",
        "  elapsed = time.time() - start_time\n",
        "  print (\"eval accuracy: \", tacc/(count*targets.data.size(0)+0.0)  )\n",
        "  #print ('eval time per batch: ', elapsed/(count+0.0))\n",
        "  return tacc/(count*targets.data.size(0)+0.0)\n",
        "\n",
        "\n",
        "def test(dh,num_batches,use_bn_trainstat=False):\n",
        "  model.eval()\n",
        "  if use_bn_trainstat:\n",
        "    model.apply(set_bn_train)\n",
        "  tacc=0\n",
        "  count=0  \n",
        "  start_time = time.time()\n",
        "  total_testdata=dh.GetDatasetSize()  \n",
        "  total_ave_acc=np.zeros((total_testdata,outputclass))\n",
        "  testlabels=np.zeros((total_testdata))\n",
        "  #print(\"number of tests: \", test_no)\n",
        "  while(1):  \n",
        "    inputs,targets,index=dh.GetBatch()\n",
        "    inputs=inputs.transpose(1,0,2,3)\n",
        "    testlabels[index]=targets\n",
        "    inputs=Variable(torch.from_numpy(inputs).cuda())\n",
        "    targets=Variable(torch.from_numpy(np.int64(targets)).cuda())\n",
        "        \n",
        "    output=model(inputs)\n",
        "    pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "    accuracy = pred.eq(targets.data).cpu().sum().numpy()    \n",
        "    total_ave_acc[index]+=output.data.cpu().numpy()\n",
        "    \n",
        "    tacc+=accuracy\n",
        "    count+=1\n",
        "    if count==global_test_no*num_batches:\n",
        "      break    \n",
        "  #total_ave_acc/=args.test_no\n",
        "  top = np.argmax(total_ave_acc, axis=-1)\n",
        "  eval_acc=np.mean(np.equal(top, testlabels))    \n",
        "  elapsed = time.time() - start_time\n",
        "  print (\"test accuracy: \", tacc/(count*targets.data.size(0)+0.0), \"eval accuracy: \", eval_acc, \"( use_bn_trainstat=\", use_bn_trainstat, \")\"  )\n",
        "  #print ('test time per batch: ', elapsed/(count+0.0))\n",
        "  return tacc/(count*targets.data.size(0)+0.0)#, eval_acc/(total_testdata+0.0)\n",
        "\n",
        "def clip_gradient(model, clip):\n",
        "    for p in model.parameters():\n",
        "        p.grad.data.clamp_(-clip,clip)\n",
        "        #print(p.size(),p.grad.data)\n",
        "\n",
        "def adjust_learning_rate(optimizer, lr):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr     \n",
        "\n",
        "def clip_weight(RNNmodel, clip):\n",
        "    for name, param in RNNmodel.named_parameters():\n",
        "      if 'weight_hh' in name:\n",
        "        param.data.clamp_(-clip,clip)\n",
        "    \n",
        "lastacc=0\n",
        "eval_dispFreq=1\n",
        "patience=0\n",
        "reduced=1\n",
        "for i in range(1,301):\n",
        "  print(\"Epoch: \", i)\n",
        "  for _ in range(num_train_batches//eval_dispFreq):\n",
        "    train(eval_dispFreq)\n",
        "  test_acc=eval(dh_eval,num_eval_batches,use_bneval)\n",
        "\n",
        "  model_clone = copy.deepcopy(model.state_dict())\n",
        "  opti_clone = copy.deepcopy(optimizer.state_dict())\n",
        "  if (test_acc >lastacc):  \n",
        "    lastacc=test_acc\n",
        "    patience=0\n",
        "  elif patience>int(pThre/reduced+0.5):\n",
        "    reduced=reduced*2\n",
        "    print ('learning rate',learning_rate)\n",
        "    model.load_state_dict(model_clone)\n",
        "    optimizer.load_state_dict(opti_clone)\n",
        "    patience=0\n",
        "    learning_rate=learning_rate*0.1\n",
        "    adjust_learning_rate(optimizer,learning_rate)     \n",
        "    if learning_rate<end_rate:\n",
        "      break  \n",
        "    test_acc=test(dh_test,num_test_batches)     \n",
        " \n",
        "  else:\n",
        "    patience+=1 \n",
        "  print('\\n')\n",
        "    \n",
        "test_acc=test(dh_test,num_test_batches)  \n",
        "test_acc=test(dh_test,num_test_batches,True)     "
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:224: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset size 2001\n",
            "Epoch:  1\n",
            "IndRNN loaded for gpu 0\n",
            "training accuracy:  0.0078125\n",
            "training accuracy:  0.015625\n",
            "training accuracy:  0.0234375\n",
            "training accuracy:  0.0078125\n",
            "training accuracy:  0.03125\n",
            "training accuracy:  0.03125\n",
            "training accuracy:  0.03125\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.015625\n",
            "training accuracy:  0.046875\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.0234375\n",
            "training accuracy:  0.03125\n",
            "training accuracy:  0.078125\n",
            "training accuracy:  0.0390625\n",
            "eval accuracy:  0.0\n",
            "\n",
            "\n",
            "Epoch:  2\n",
            "training accuracy:  0.046875\n",
            "training accuracy:  0.0546875\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.03125\n",
            "training accuracy:  0.0234375\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.046875\n",
            "training accuracy:  0.0703125\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.03125\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.0625\n",
            "training accuracy:  0.03125\n",
            "training accuracy:  0.0078125\n",
            "eval accuracy:  0.0078125\n",
            "\n",
            "\n",
            "Epoch:  3\n",
            "training accuracy:  0.0546875\n",
            "training accuracy:  0.0625\n",
            "training accuracy:  0.078125\n",
            "training accuracy:  0.0546875\n",
            "training accuracy:  0.0234375\n",
            "training accuracy:  0.03125\n",
            "training accuracy:  0.0234375\n",
            "training accuracy:  0.078125\n",
            "training accuracy:  0.015625\n",
            "training accuracy:  0.0625\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.03125\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.0234375\n",
            "training accuracy:  0.09375\n",
            "eval accuracy:  0.009375\n",
            "\n",
            "\n",
            "Epoch:  4\n",
            "training accuracy:  0.0859375\n",
            "training accuracy:  0.0625\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.0546875\n",
            "training accuracy:  0.0859375\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.0703125\n",
            "training accuracy:  0.0546875\n",
            "training accuracy:  0.0859375\n",
            "training accuracy:  0.046875\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.046875\n",
            "training accuracy:  0.0546875\n",
            "training accuracy:  0.09375\n",
            "eval accuracy:  0.0\n",
            "\n",
            "\n",
            "Epoch:  5\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.0625\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.046875\n",
            "training accuracy:  0.046875\n",
            "training accuracy:  0.125\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.015625\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.046875\n",
            "training accuracy:  0.078125\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.03125\n",
            "training accuracy:  0.0234375\n",
            "training accuracy:  0.078125\n",
            "eval accuracy:  0.0\n",
            "\n",
            "\n",
            "Epoch:  6\n",
            "training accuracy:  0.0625\n",
            "training accuracy:  0.078125\n",
            "training accuracy:  0.0859375\n",
            "training accuracy:  0.0859375\n",
            "training accuracy:  0.0625\n",
            "training accuracy:  0.078125\n",
            "training accuracy:  0.0859375\n",
            "training accuracy:  0.0859375\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.109375\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.046875\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.078125\n",
            "training accuracy:  0.0859375\n",
            "eval accuracy:  0.028125\n",
            "\n",
            "\n",
            "Epoch:  7\n",
            "training accuracy:  0.1328125\n",
            "training accuracy:  0.0625\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.0625\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.0625\n",
            "training accuracy:  0.0390625\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.0546875\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.0859375\n",
            "training accuracy:  0.0625\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.0859375\n",
            "training accuracy:  0.078125\n",
            "eval accuracy:  0.075\n",
            "\n",
            "\n",
            "Epoch:  8\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.1328125\n",
            "training accuracy:  0.0859375\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.109375\n",
            "training accuracy:  0.078125\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.125\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.125\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.0703125\n",
            "training accuracy:  0.078125\n",
            "eval accuracy:  0.0453125\n",
            "\n",
            "\n",
            "Epoch:  9\n",
            "training accuracy:  0.1328125\n",
            "training accuracy:  0.140625\n",
            "training accuracy:  0.1328125\n",
            "training accuracy:  0.1328125\n",
            "training accuracy:  0.1328125\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.125\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.078125\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.1484375\n",
            "training accuracy:  0.1328125\n",
            "training accuracy:  0.1484375\n",
            "eval accuracy:  0.05625\n",
            "\n",
            "\n",
            "Epoch:  10\n",
            "training accuracy:  0.140625\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.2265625\n",
            "eval accuracy:  0.05\n",
            "\n",
            "\n",
            "Epoch:  11\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.1484375\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.1484375\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.125\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.1484375\n",
            "training accuracy:  0.1328125\n",
            "training accuracy:  0.109375\n",
            "training accuracy:  0.125\n",
            "training accuracy:  0.125\n",
            "training accuracy:  0.1484375\n",
            "training accuracy:  0.15625\n",
            "eval accuracy:  0.0921875\n",
            "\n",
            "\n",
            "Epoch:  12\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.140625\n",
            "training accuracy:  0.140625\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.109375\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.140625\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.09375\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.171875\n",
            "eval accuracy:  0.075\n",
            "\n",
            "\n",
            "Epoch:  13\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.1015625\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.1328125\n",
            "training accuracy:  0.140625\n",
            "training accuracy:  0.1328125\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.234375\n",
            "eval accuracy:  0.1328125\n",
            "\n",
            "\n",
            "Epoch:  14\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.2109375\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.140625\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.109375\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.109375\n",
            "training accuracy:  0.140625\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.15625\n",
            "eval accuracy:  0.0796875\n",
            "\n",
            "\n",
            "Epoch:  15\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.140625\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.1484375\n",
            "training accuracy:  0.2421875\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.2265625\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.234375\n",
            "eval accuracy:  0.1109375\n",
            "\n",
            "\n",
            "Epoch:  16\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.1171875\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.1796875\n",
            "eval accuracy:  0.0828125\n",
            "\n",
            "\n",
            "Epoch:  17\n",
            "training accuracy:  0.2265625\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.203125\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.1328125\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.2109375\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.140625\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.2421875\n",
            "eval accuracy:  0.115625\n",
            "\n",
            "\n",
            "Epoch:  18\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.2109375\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.1640625\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.15625\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.171875\n",
            "training accuracy:  0.2265625\n",
            "eval accuracy:  0.153125\n",
            "\n",
            "\n",
            "Epoch:  19\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.2578125\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.2265625\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.203125\n",
            "training accuracy:  0.2109375\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.296875\n",
            "eval accuracy:  0.1296875\n",
            "\n",
            "\n",
            "Epoch:  20\n",
            "training accuracy:  0.2109375\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.203125\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.2421875\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.2265625\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.234375\n",
            "eval accuracy:  0.078125\n",
            "\n",
            "\n",
            "Epoch:  21\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.2265625\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.2578125\n",
            "training accuracy:  0.2734375\n",
            "eval accuracy:  0.1375\n",
            "\n",
            "\n",
            "Epoch:  22\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.2109375\n",
            "training accuracy:  0.2578125\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.2421875\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.203125\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.2421875\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.203125\n",
            "training accuracy:  0.1875\n",
            "training accuracy:  0.25\n",
            "eval accuracy:  0.18125\n",
            "\n",
            "\n",
            "Epoch:  23\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.2265625\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.2578125\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.2578125\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.2421875\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.2578125\n",
            "training accuracy:  0.265625\n",
            "eval accuracy:  0.2515625\n",
            "\n",
            "\n",
            "Epoch:  24\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.203125\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.2578125\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.2109375\n",
            "training accuracy:  0.2109375\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.2890625\n",
            "eval accuracy:  0.1484375\n",
            "\n",
            "\n",
            "Epoch:  25\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.28125\n",
            "eval accuracy:  0.1125\n",
            "\n",
            "\n",
            "Epoch:  26\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.2109375\n",
            "training accuracy:  0.1796875\n",
            "training accuracy:  0.2421875\n",
            "training accuracy:  0.203125\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.2109375\n",
            "training accuracy:  0.234375\n",
            "eval accuracy:  0.19375\n",
            "\n",
            "\n",
            "Epoch:  27\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.2578125\n",
            "eval accuracy:  0.2203125\n",
            "\n",
            "\n",
            "Epoch:  28\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.2421875\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.1953125\n",
            "training accuracy:  0.2265625\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.2421875\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.34375\n",
            "eval accuracy:  0.1625\n",
            "\n",
            "\n",
            "Epoch:  29\n",
            "training accuracy:  0.2578125\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.2578125\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.203125\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.2734375\n",
            "eval accuracy:  0.2234375\n",
            "\n",
            "\n",
            "Epoch:  30\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.21875\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.3984375\n",
            "eval accuracy:  0.184375\n",
            "\n",
            "\n",
            "Epoch:  31\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.3125\n",
            "eval accuracy:  0.215625\n",
            "\n",
            "\n",
            "Epoch:  32\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.2578125\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.359375\n",
            "eval accuracy:  0.1578125\n",
            "\n",
            "\n",
            "Epoch:  33\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.2734375\n",
            "eval accuracy:  0.225\n",
            "\n",
            "\n",
            "Epoch:  34\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.234375\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.3515625\n",
            "eval accuracy:  0.196875\n",
            "\n",
            "\n",
            "Epoch:  35\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.375\n",
            "eval accuracy:  0.253125\n",
            "\n",
            "\n",
            "Epoch:  36\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.25\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.359375\n",
            "eval accuracy:  0.275\n",
            "\n",
            "\n",
            "Epoch:  37\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.3828125\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.3828125\n",
            "training accuracy:  0.328125\n",
            "eval accuracy:  0.2109375\n",
            "\n",
            "\n",
            "Epoch:  38\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.3984375\n",
            "eval accuracy:  0.2390625\n",
            "\n",
            "\n",
            "Epoch:  39\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.3046875\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.3125\n",
            "eval accuracy:  0.2328125\n",
            "\n",
            "\n",
            "Epoch:  40\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.28125\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.265625\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.3203125\n",
            "training accuracy:  0.359375\n",
            "eval accuracy:  0.284375\n",
            "\n",
            "\n",
            "Epoch:  41\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.3828125\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.3125\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.3515625\n",
            "eval accuracy:  0.2265625\n",
            "\n",
            "\n",
            "Epoch:  42\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.3828125\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.40625\n",
            "eval accuracy:  0.259375\n",
            "\n",
            "\n",
            "Epoch:  43\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.3984375\n",
            "eval accuracy:  0.2328125\n",
            "\n",
            "\n",
            "Epoch:  44\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.2734375\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.40625\n",
            "eval accuracy:  0.1953125\n",
            "\n",
            "\n",
            "Epoch:  45\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.2890625\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.3828125\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.4296875\n",
            "eval accuracy:  0.2859375\n",
            "\n",
            "\n",
            "Epoch:  46\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.328125\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.3828125\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.40625\n",
            "eval accuracy:  0.190625\n",
            "\n",
            "\n",
            "Epoch:  47\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.296875\n",
            "training accuracy:  0.3671875\n",
            "eval accuracy:  0.240625\n",
            "\n",
            "\n",
            "Epoch:  48\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.3359375\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.3828125\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.3515625\n",
            "eval accuracy:  0.1203125\n",
            "\n",
            "\n",
            "Epoch:  49\n",
            "training accuracy:  0.34375\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.453125\n",
            "eval accuracy:  0.2453125\n",
            "\n",
            "\n",
            "Epoch:  50\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.3828125\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.546875\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.46875\n",
            "eval accuracy:  0.21875\n",
            "\n",
            "\n",
            "Epoch:  51\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.3828125\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.40625\n",
            "eval accuracy:  0.2625\n",
            "\n",
            "\n",
            "Epoch:  52\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.3828125\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.4453125\n",
            "eval accuracy:  0.25\n",
            "\n",
            "\n",
            "Epoch:  53\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.4765625\n",
            "eval accuracy:  0.259375\n",
            "\n",
            "\n",
            "Epoch:  54\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.5625\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.4609375\n",
            "eval accuracy:  0.2421875\n",
            "\n",
            "\n",
            "Epoch:  55\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.4296875\n",
            "eval accuracy:  0.2765625\n",
            "\n",
            "\n",
            "Epoch:  56\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.4140625\n",
            "eval accuracy:  0.3078125\n",
            "\n",
            "\n",
            "Epoch:  57\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.5703125\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.4375\n",
            "eval accuracy:  0.225\n",
            "\n",
            "\n",
            "Epoch:  58\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.3515625\n",
            "training accuracy:  0.609375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.3671875\n",
            "eval accuracy:  0.265625\n",
            "\n",
            "\n",
            "Epoch:  59\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.390625\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.3828125\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.375\n",
            "training accuracy:  0.546875\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.453125\n",
            "eval accuracy:  0.2671875\n",
            "\n",
            "\n",
            "Epoch:  60\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.3984375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.546875\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.375\n",
            "eval accuracy:  0.25625\n",
            "\n",
            "\n",
            "Epoch:  61\n",
            "training accuracy:  0.359375\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.3671875\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.546875\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.5\n",
            "eval accuracy:  0.2703125\n",
            "\n",
            "\n",
            "Epoch:  62\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.578125\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.46875\n",
            "eval accuracy:  0.35625\n",
            "\n",
            "\n",
            "Epoch:  63\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.5703125\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.4140625\n",
            "training accuracy:  0.515625\n",
            "eval accuracy:  0.2953125\n",
            "\n",
            "\n",
            "Epoch:  64\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.5625\n",
            "training accuracy:  0.4375\n",
            "eval accuracy:  0.28125\n",
            "\n",
            "\n",
            "Epoch:  65\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.5390625\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.5859375\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.578125\n",
            "training accuracy:  0.453125\n",
            "eval accuracy:  0.2109375\n",
            "\n",
            "\n",
            "Epoch:  66\n",
            "training accuracy:  0.546875\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.5859375\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.59375\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.546875\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.5\n",
            "eval accuracy:  0.309375\n",
            "\n",
            "\n",
            "Epoch:  67\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.4296875\n",
            "training accuracy:  0.5625\n",
            "training accuracy:  0.5859375\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.5703125\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.4375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.53125\n",
            "eval accuracy:  0.2453125\n",
            "\n",
            "\n",
            "Epoch:  68\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.5625\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.6015625\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.6015625\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.40625\n",
            "training accuracy:  0.5625\n",
            "training accuracy:  0.5390625\n",
            "training accuracy:  0.484375\n",
            "eval accuracy:  0.29375\n",
            "\n",
            "\n",
            "Epoch:  69\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.421875\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.6015625\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.59375\n",
            "training accuracy:  0.5859375\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.4296875\n",
            "eval accuracy:  0.259375\n",
            "\n",
            "\n",
            "Epoch:  70\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.546875\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.578125\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.5390625\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.5703125\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.515625\n",
            "eval accuracy:  0.2796875\n",
            "\n",
            "\n",
            "Epoch:  71\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.5625\n",
            "training accuracy:  0.5703125\n",
            "training accuracy:  0.5859375\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.5390625\n",
            "training accuracy:  0.546875\n",
            "training accuracy:  0.5625\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.46875\n",
            "eval accuracy:  0.2875\n",
            "\n",
            "\n",
            "Epoch:  72\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.5390625\n",
            "training accuracy:  0.4453125\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.5859375\n",
            "training accuracy:  0.5390625\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.46875\n",
            "training accuracy:  0.5703125\n",
            "training accuracy:  0.59375\n",
            "training accuracy:  0.578125\n",
            "training accuracy:  0.5\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.46875\n",
            "eval accuracy:  0.2703125\n",
            "\n",
            "\n",
            "Epoch:  73\n",
            "training accuracy:  0.578125\n",
            "training accuracy:  0.4921875\n",
            "training accuracy:  0.5703125\n",
            "training accuracy:  0.5390625\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.5390625\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.515625\n",
            "eval accuracy:  0.2578125\n",
            "\n",
            "\n",
            "Epoch:  74\n",
            "training accuracy:  0.5234375\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.4765625\n",
            "training accuracy:  0.453125\n",
            "training accuracy:  0.484375\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.5546875\n",
            "training accuracy:  0.4609375\n",
            "training accuracy:  0.5078125\n",
            "training accuracy:  0.578125\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.53125\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.515625\n",
            "training accuracy:  0.5078125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-e871c7779a49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_batches\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0meval_dispFreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dispFreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m   \u001b[0mtest_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_eval_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_bneval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m   \u001b[0mmodel_clone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-148-e871c7779a49>\u001b[0m in \u001b[0;36meval\u001b[0;34m(dh, num_batches, use_bn_trainstat)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-144-8a2adbacd4e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    111\u001b[0m           \u001b[0mrnnoutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rnnlayer%d'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBNs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnnoutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rnnlayer%d'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mrnnoutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rnnlayer%d'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdropout_overtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnnoutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rnnlayer%d'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnnoutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rnnlayer%d'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNNs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-144-8a2adbacd4e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, p, training)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#torch.ones_like(input[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "vhfI-QEPolFC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if test_CV:\n",
        "  save_name='indrnn_action_model_CV'+str(outputclass)\n",
        "else:\n",
        "  save_name='indrnn_action_model_CS'+str(outputclass)\n",
        "torch.save(model, save_name)\n",
        "model_file = drive.CreateFile({'title' : save_name})\n",
        "model_file.SetContentFile(save_name)\n",
        "model_file.Upload()\n",
        "\n",
        "# download to google drive\n",
        "drive.CreateFile({'id': model_file.get('id')})"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}