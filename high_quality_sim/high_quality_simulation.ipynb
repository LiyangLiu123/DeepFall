{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "high_quality_simulation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiyangLiu123/DeepFall/blob/master/high_quality_sim/high_quality_simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeK_B3f9uxed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# def train_opts(parser):\n",
        "#   parser.add_argument('--lr', type=float, default=2e-4,help='lr')\n",
        "#   parser.add_argument('--batch_size', type=int, default=128, help='batch_size')\n",
        "#   parser.add_argument('--seq_len', type=int, default=20)\n",
        "#   parser.add_argument('--num_layers', type=int, default=6,help='num_layers')\n",
        "#   parser.add_argument('--hidden_size', type=int, default=512)\n",
        "#   parser.add_argument('--test_CV', action='store_true', default=False,help='use the CS test setting. If True, then use CV test setting.')\n",
        "#   parser.add_argument('--use_weightdecay_nohiddenW', action='store_true', default=False)\n",
        "#   parser.add_argument('--decayfactor', type=float, default=1e-4,help='lr')\n",
        "#   parser.add_argument('--opti', type=str, default='adam')\n",
        "#   parser.add_argument('--pThre', type=int, default=20)\n",
        "#   parser.add_argument('--test_no', type=int, default=20)\n",
        "\n",
        "#   parser.add_argument('--ini_in2hid', type=float, default=0.002)\n",
        "\n",
        "#   parser.add_argument('--constrain_U', action='store_true', default=False)\n",
        "#   parser.add_argument('--MAG', type=float, default=5.0)\n",
        "\n",
        "#   parser.add_argument('--eval_fold', type=int, default=5)\n",
        "#   parser.add_argument('--use_bneval', action='store_true', default=False)\n",
        "#   parser.add_argument('--ini_b', type=float, default=0.0)\n",
        "#   parser.add_argument('--end_rate', type=float, default=1e-6)\n",
        "  \n",
        "#   parser.add_argument('--dropout', type=float, default=0.1,help='lr')\n",
        "  \n",
        "lr = 2e-4\n",
        "batch_size = 128\n",
        "seq_len = 90\n",
        "num_layers = 6\n",
        "hidden_size = 512\n",
        "use_weightdecay_nohiddenW = False\n",
        "decay_factor = 1e-4\n",
        "opti = 'adam'\n",
        "pThre = 20\n",
        "global_test_no = 1\n",
        "ini_in2hid = 0.002\n",
        "constrain_U = False\n",
        "MAG = 5.0\n",
        "eval_fold = 5\n",
        "use_bneval = False\n",
        "ini_b = 0.0\n",
        "end_rate = 1e-6\n",
        "\n",
        "dropout = 0.25\n",
        "  \n",
        "n_dimension = 2\n",
        "num_joints = 18 # two skeletons with each 25 joints\n",
        "outputclass = 2\n",
        "\n",
        "\n",
        "train_link = 'https://drive.google.com/open?id=1u0yQ9_HpbSWr9bA_kk9JSYIEiIDtl0o0'\n",
        "train_len_link = 'https://drive.google.com/open?id=1U2cpA5nAna6if8defEE47yM4NzNmYR5Q'\n",
        "train_label_link = 'https://drive.google.com/open?id=1Yc_48Q1YNMuCXD0ZOGAUyYz8vuGT9wIC'\n",
        "test_link = 'https://drive.google.com/open?id=1AQoAmDJiiamzk40g8xT3Ew1ZGfZqvwqw'\n",
        "test_len_link = 'https://drive.google.com/open?id=1o6l6psaYxUEEz1ShRo-dbc54Zh5CsfFj'\n",
        "test_label_link = 'https://drive.google.com/open?id=1pszo7gtCFm0nhKJRqdKW4fpNv2HJpwaf'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il_6gDwCu3PI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This code is to implement the IndRNN (only the recurrent part). The code is based on the implementation from \n",
        "https://github.com/StefOe/indrnn-pytorch/blob/master/indrnn.py.\n",
        "Since this only contains the recurrent part of IndRNN, fully connected layers or convolutional layers are needed before it.\n",
        "Please cite the following paper if you find it useful.\n",
        "Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. \"Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN,\" \n",
        "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5457-5466. 2018.\n",
        "@inproceedings{li2018independently,\n",
        "  title={Independently recurrent neural network (indrnn): Building A longer and deeper RNN},\n",
        "  author={Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},\n",
        "  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n",
        "  pages={5457--5466},\n",
        "  year={2018}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import Parameter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "\n",
        "\n",
        "class IndRNNCell_onlyrecurrent(nn.Module):\n",
        "    r\"\"\"An IndRNN cell with ReLU non-linearity. This is only the recurrent part where the input is already processed with w_{ih} * x + b_{ih}.\n",
        "\n",
        "    .. math::\n",
        "        input=w_{ih} * x + b_{ih}\n",
        "        h' = \\relu(input +  w_{hh} (*) h)\n",
        "    With (*) being element-wise vector multiplication.\n",
        "\n",
        "    Args:\n",
        "        hidden_size: The number of features in the hidden state h\n",
        "\n",
        "    Inputs: input, hidden\n",
        "        - **input** (batch, input_size): tensor containing input features\n",
        "        - **hidden** (batch, hidden_size): tensor containing the initial hidden\n",
        "          state for each element in the batch.\n",
        "\n",
        "    Outputs: h'\n",
        "        - **h'** (batch, hidden_size): tensor containing the next hidden state\n",
        "          for each element in the batch\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, \n",
        "                 hidden_max_abs=None, recurrent_init=None):\n",
        "        super(IndRNNCell_onlyrecurrent, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.recurrent_init = recurrent_init\n",
        "        self.weight_hh = Parameter(torch.Tensor(hidden_size))            \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for name, weight in self.named_parameters():\n",
        "            if \"weight_hh\" in name:\n",
        "                if self.recurrent_init is None:\n",
        "                    nn.init.uniform(weight, a=0, b=1)\n",
        "                else:\n",
        "                    self.recurrent_init(weight)\n",
        "\n",
        "    def forward(self, input, hx):\n",
        "        return F.relu(input + hx * self.weight_hh.unsqueeze(0).expand(hx.size(0), len(self.weight_hh)))\n",
        "\n",
        "\n",
        "class IndRNN_onlyrecurrent(nn.Module):\n",
        "    r\"\"\"Applies an IndRNN with `ReLU` non-linearity to an input sequence. \n",
        "    This is only the recurrent part where the input is already processed with w_{ih} * x + b_{ih}.\n",
        "\n",
        "\n",
        "    For each element in the input sequence, each layer computes the following\n",
        "    function:\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        h_t = \\relu(input_t +  w_{hh} (*) h_{(t-1)})\n",
        "\n",
        "    where :math:`h_t` is the hidden state at time `t`, and :math:`input_t`\n",
        "    is the input at time `t`. (*) is element-wise multiplication.\n",
        "\n",
        "    Args:\n",
        "        hidden_size: The number of features in the hidden state `h`\n",
        "        batch_first: If ``True``, then the input and output tensors are provided\n",
        "            as `(batch, seq, feature)`\n",
        "\n",
        "    Inputs: input, h_0\n",
        "        - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features\n",
        "          of the input sequence. The input can also be a packed variable length\n",
        "          sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`\n",
        "          or :func:`torch.nn.utils.rnn.pack_sequence`\n",
        "          for details.\n",
        "        - **h_0** of shape `(num_directions, batch, hidden_size)`: tensor\n",
        "          containing the initial hidden state for each element in the batch.\n",
        "          Defaults to zero if not provided.\n",
        "\n",
        "    Outputs: output, h_n\n",
        "        - **output** of shape `(seq_len, batch, hidden_size * num_directions)`: tensor\n",
        "          containing the output features (`h_k`) from the last layer of the RNN,\n",
        "          for each `k`.  If a :class:`torch.nn.utils.rnn.PackedSequence` has\n",
        "          been given as the input, the output will also be a packed sequence.\n",
        "        - **h_n** (num_directions, batch, hidden_size): tensor\n",
        "          containing the hidden state for `k = seq_len`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, \n",
        "                 batch_first=False, bidirectional=False, recurrent_inits=None,\n",
        "                 **kwargs):\n",
        "        super(IndRNN_onlyrecurrent, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_first = batch_first\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        num_directions = 2 if self.bidirectional else 1\n",
        "\n",
        "        if batch_first:\n",
        "            self.time_index = 1\n",
        "            self.batch_index = 0\n",
        "        else:\n",
        "            self.time_index = 0\n",
        "            self.batch_index = 1\n",
        "\n",
        "        cells = []\n",
        "        directions = []\n",
        "        if recurrent_inits is not None:\n",
        "            kwargs[\"recurrent_init\"] = recurrent_inits\n",
        "        for dir in range(num_directions):\n",
        "            directions.append(IndRNNCell_onlyrecurrent(hidden_size, **kwargs))\n",
        "        self.cells = nn.ModuleList(directions)\n",
        "\n",
        "        h0 = torch.zeros(hidden_size * num_directions)\n",
        "        self.register_buffer('h0', torch.autograd.Variable(h0))\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        time_index = self.time_index\n",
        "        batch_index = self.batch_index\n",
        "        num_directions = 2 if self.bidirectional else 1\n",
        "        hidden_init = self.h0.unsqueeze(0).expand(\n",
        "            x.size(batch_index),\n",
        "            self.hidden_size * num_directions).contiguous()\n",
        "\n",
        "        x_n = []\n",
        "        for dir, cell in enumerate(self.cells):\n",
        "            hx_cell = hidden_init[\n",
        "                :, self.hidden_size * dir: self.hidden_size * (dir + 1)]\n",
        "            outputs = []\n",
        "            hiddens = []\n",
        "            x_T = torch.unbind(x, time_index)\n",
        "            if dir == 1:\n",
        "                x_T = reversed(x_T)\n",
        "            for x_t in x_T:\n",
        "                hx_cell = cell(x_t, hx_cell)\n",
        "                outputs.append(hx_cell)\n",
        "            if dir == 1:\n",
        "                outputs = outputs[::-1]\n",
        "            x_cell = torch.stack(outputs, time_index)\n",
        "            x_n.append(x_cell)\n",
        "            hiddens.append(hx_cell)\n",
        "        x = torch.cat(x_n, -1)\n",
        "        return x.squeeze(2), torch.cat(hiddens, -1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC-4r4R3pPRh",
        "colab_type": "code",
        "outputId": "ca23fe79-4f1b-473c-cb90-33a99eb9ccc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "fluff, train_id = train_link.split('=')\n",
        "print (train_id) # Verify that you have everything after '='\n",
        "\n",
        "fluff, train_len_id = train_len_link.split('=')\n",
        "print (train_len_id) # Verify that you have everything after '='\n",
        "\n",
        "fluff, train_label_id = train_label_link.split('=')\n",
        "print (train_label_id) # Verify that you have everything after '='\n",
        "\n",
        "fluff, test_id = test_link.split('=')\n",
        "print (test_id) # Verify that you have everything after '='\n",
        "\n",
        "fluff, test_len_id = test_len_link.split('=')\n",
        "print (test_len_id) # Verify that you have everything after '='\n",
        "\n",
        "fluff, test_label_id = test_label_link.split('=')\n",
        "print (test_label_id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':train_id}) \n",
        "downloaded.GetContentFile('train.npy')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':train_len_id}) \n",
        "downloaded.GetContentFile('train_len.npy')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':train_label_id}) \n",
        "downloaded.GetContentFile('train_label.npy')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':test_id}) \n",
        "downloaded.GetContentFile('test.npy') \n",
        "\n",
        "downloaded = drive.CreateFile({'id':test_len_id}) \n",
        "downloaded.GetContentFile('test_len.npy') \n",
        "\n",
        "downloaded = drive.CreateFile({'id':test_label_id}) \n",
        "downloaded.GetContentFile('test_label.npy') \n",
        "\n",
        "train_datasets='train'\n",
        "test_dataset='test'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1u0yQ9_HpbSWr9bA_kk9JSYIEiIDtl0o0\n",
            "1U2cpA5nAna6if8defEE47yM4NzNmYR5Q\n",
            "1Yc_48Q1YNMuCXD0ZOGAUyYz8vuGT9wIC\n",
            "1AQoAmDJiiamzk40g8xT3Ew1ZGfZqvwqw\n",
            "1o6l6psaYxUEEz1ShRo-dbc54Zh5CsfFj\n",
            "1pszo7gtCFm0nhKJRqdKW4fpNv2HJpwaf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ_zpZj0vDGg",
        "colab_type": "code",
        "outputId": "c8e04061-37ea-405b-ca6b-521e1be03946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\"\"\"\n",
        "This code is to implement the IndRNN (only the recurrent part) using CUDA for fast computation. The CUDA part is similar the SRU implementation from \n",
        "https://github.com/taolei87/sru.\n",
        "This runs around 32 times faster than the general pytorch implementation on pixel MNIST example (sequence lengeth 784). For longer sequence, \n",
        "it will be even more efficient, and vice versa. \n",
        "Since this only contains the recurrent part of IndRNN, fully connected layers or convolutional layers are needed before it.\n",
        "Please cite the following paper if you find it useful.\n",
        "Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. \"Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN,\" \n",
        "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5457-5466. 2018.\n",
        "@inproceedings{li2018independently,\n",
        "  title={Independently recurrent neural network (indrnn): Building A longer and deeper RNN},\n",
        "  author={Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},\n",
        "  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n",
        "  pages={5457--5466},\n",
        "  year={2018}\n",
        "}\n",
        "\"\"\"\n",
        "!pip install cupy\n",
        "!pip install pynvrtc\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "#import warnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "\n",
        "from torch.nn import Parameter\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "from cupy.cuda import function\n",
        "from pynvrtc.compiler import Program\n",
        "from collections import namedtuple\n",
        "\n",
        "IndRNN_CODE = \"\"\"\n",
        "extern \"C\" {\n",
        "\n",
        "    __forceinline__ __device__ float reluf(float x)\n",
        "    {\n",
        "        return (x > 0.f) ? x : 0.f;\n",
        "    }\n",
        "\n",
        "    __forceinline__ __device__ float calc_grad_activation(float x)\n",
        "    {\n",
        "        return (x > 0.f) ? 1.f : 0.f;\n",
        "    }\n",
        "\n",
        "    __global__ void indrnn_fwd( const float * __restrict__ x,\n",
        "                            const float * __restrict__ weight_hh, const float * __restrict__ h0,\n",
        "                            const int len, const int batch, const int hidden_size, \n",
        "                            float * __restrict__ h)\n",
        "    {\n",
        "        int ncols = batch*hidden_size;\n",
        "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "        if (col >= ncols) return;       \n",
        "        const float weight_hh_cur = *(weight_hh + (col%hidden_size));\n",
        "        float cur = *(h0 + col);\n",
        "        const float *xp = x+col;\n",
        "        float *hp = h+col;\n",
        "\n",
        "        for (int row = 0; row < len; ++row)\n",
        "        {\n",
        "            cur=reluf(cur*weight_hh_cur+(*xp));\n",
        "            *hp=cur;\n",
        "            xp += ncols;\n",
        "            hp += ncols;            \n",
        "        }\n",
        "    }\n",
        "\n",
        "    __global__ void indrnn_bwd(const float * __restrict__ x,\n",
        "                             const float * __restrict__ weight_hh, const float * __restrict__ h0,\n",
        "                             const float * __restrict__ h,\n",
        "                            const float * __restrict__ grad_h, \n",
        "                            const int len, const int batch, const int hidden_size, \n",
        "                            float * __restrict__ grad_x,\n",
        "                            float * __restrict__ grad_weight_hh, float * __restrict__ grad_h0)\n",
        "    {    \n",
        "        int ncols = batch*hidden_size;\n",
        "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "        if (col >= ncols) return;        \n",
        "        const float weight_hh_cur = *(weight_hh + (col%hidden_size));\n",
        "        float gweight_hh_cur = 0;\n",
        "        float cur = 0;  // *(grad_last + col);        //0; strange gradient behavior. grad_last and grad_h, one of them is zero.     \n",
        "        \n",
        "        const float *xp = x+col + (len-1)*ncols;\n",
        "        const float *hp = h+col + (len-1)*ncols;      \n",
        "        float *gxp = grad_x + col + (len-1)*ncols;\n",
        "        const float *ghp = grad_h + col + (len-1)*ncols;\n",
        "        \n",
        "\n",
        "        for (int row = len-1; row >= 0; --row)\n",
        "        {        \n",
        "            const float prev_h_val = (row>0) ? (*(hp-ncols)) : (*(h0+col));\n",
        "            //float h_val_beforeact = prev_h_val*weight_hh_cur+(*xp);\n",
        "            float gh_beforeact = ((*ghp) + cur)*calc_grad_activation(prev_h_val*weight_hh_cur+(*xp));\n",
        "            cur = gh_beforeact*weight_hh_cur;\n",
        "            gweight_hh_cur += gh_beforeact*prev_h_val;\n",
        "            *gxp = gh_beforeact;\n",
        "\n",
        "            xp -= ncols;\n",
        "            hp -= ncols;\n",
        "            gxp -= ncols;\n",
        "            ghp -= ncols;        \n",
        "        }\n",
        "\n",
        "        atomicAdd(grad_weight_hh + (col%hidden_size), gweight_hh_cur);\n",
        "        *(grad_h0 +col) = cur;\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class IndRNN_Compute_GPU(Function):\n",
        "\n",
        "    _IndRNN_PROG = Program(IndRNN_CODE, 'indrnn_prog.cu')#.encode('utf-8')  .encode()\n",
        "    _IndRNN_PTX = _IndRNN_PROG.compile()\n",
        "    _DEVICE2FUNC = {}\n",
        "\n",
        "    def __init__(self,gradclipvalue=0):\n",
        "        super(IndRNN_Compute_GPU, self).__init__()\n",
        "        self.gradclipvalue=gradclipvalue\n",
        "\n",
        "    def compile_functions(self):\n",
        "        device = torch.cuda.current_device()\n",
        "        print ('IndRNN loaded for gpu {}'.format(device))\n",
        "        mod = function.Module()\n",
        "        mod.load(bytes(self._IndRNN_PTX.encode()))\n",
        "        fwd_func = mod.get_function('indrnn_fwd')\n",
        "        bwd_func = mod.get_function('indrnn_bwd')\n",
        "\n",
        "        Stream = namedtuple('Stream', ['ptr'])\n",
        "        current_stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n",
        "\n",
        "        self._DEVICE2FUNC[device] = (current_stream, fwd_func, bwd_func)\n",
        "        return current_stream, fwd_func, bwd_func\n",
        "\n",
        "    def get_functions(self):\n",
        "        res = self._DEVICE2FUNC.get(torch.cuda.current_device(), None)\n",
        "        return res if res else self.compile_functions()\n",
        "\n",
        "    def forward(self, x, weight_hh, h0):\n",
        "        length = x.size(0) if x.dim() == 3 else 1\n",
        "        batch = x.size(-2)\n",
        "        hidden_size = x.size(-1)  #hidden_size\n",
        "        ncols = batch*hidden_size\n",
        "        thread_per_block = min(512, ncols)\n",
        "        num_block = (ncols-1)//thread_per_block+1\n",
        "        \n",
        "        size = (length, batch, hidden_size) if x.dim() == 3 else (batch, hidden_size)\n",
        "        h = x.new(*size)\n",
        "\n",
        "        stream, fwd_func, _ = self.get_functions()\n",
        "        FUNC = fwd_func\n",
        "        FUNC(args=[\n",
        "            x.contiguous().data_ptr(),\n",
        "            weight_hh.contiguous().data_ptr(),\n",
        "            h0.contiguous().data_ptr(),\n",
        "            length,\n",
        "            batch,\n",
        "            hidden_size,\n",
        "            h.contiguous().data_ptr()],\n",
        "            block = (thread_per_block,1,1), grid = (num_block,1,1),\n",
        "            stream=stream\n",
        "        )\n",
        "\n",
        "        self.save_for_backward(x, h, weight_hh, h0)#\n",
        "        return h\n",
        "\n",
        "    def backward(self, grad_h):\n",
        "        x, h, weight_hh, h0 = self.saved_tensors\n",
        "        length = x.size(0) if x.dim() == 3 else 1\n",
        "        batch = x.size(-2)\n",
        "        hidden_size = x.size(-1)#self.hidden_size\n",
        "        ncols = batch*hidden_size\n",
        "        thread_per_block = min(512, ncols)\n",
        "        num_block = (ncols-1)//thread_per_block+1\n",
        "\n",
        "        grad_x = x.new(*x.size())\n",
        "        grad_weight_hh = x.new(hidden_size).zero_()\n",
        "        grad_h0 = x.new(batch, hidden_size)  \n",
        "\n",
        "        stream, _, bwd_func = self.get_functions()\n",
        "        FUNC = bwd_func\n",
        "        FUNC(args=[\n",
        "            x.contiguous().data_ptr(),\n",
        "            weight_hh.contiguous().data_ptr(),\n",
        "            h0.contiguous().data_ptr(),\n",
        "            h.contiguous().data_ptr(),\n",
        "            grad_h.contiguous().data_ptr(),\n",
        "            length,\n",
        "            batch,\n",
        "            hidden_size,\n",
        "            grad_x.contiguous().data_ptr(),\n",
        "            grad_weight_hh.contiguous().data_ptr(),\n",
        "            grad_h0.contiguous().data_ptr()],\n",
        "            block = (thread_per_block,1,1), grid = (num_block,1,1),\n",
        "            stream=stream\n",
        "        )\n",
        "        if self.gradclipvalue>0:\n",
        "            grad_x.clamp_(-self.gradclipvalue,self.gradclipvalue)\n",
        "            grad_weight_hh.clamp_(-self.gradclipvalue,self.gradclipvalue)\n",
        "            grad_h0.clamp_(-self.gradclipvalue,self.gradclipvalue)\n",
        "        return grad_x, grad_weight_hh, grad_h0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class cuda_IndRNN_onlyrecurrent(nn.Module):\n",
        "    def __init__(self, hidden_size, gradclipvalue=0,\n",
        "                 hidden_max_abs=None, recurrent_init=None):\n",
        "        super(cuda_IndRNN_onlyrecurrent, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.recurrent_init = recurrent_init\n",
        "        self.weight_hh = Parameter(torch.Tensor(hidden_size))   \n",
        "        self.gradclipvalue=gradclipvalue         \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for name, weight in self.named_parameters():\n",
        "            if \"weight_hh\" in name:\n",
        "                if self.recurrent_init is None:\n",
        "                    nn.init.uniform(weight, a=0, b=1)\n",
        "                else:\n",
        "                    self.recurrent_init(weight)\n",
        "\n",
        "    def forward(self, input, h0=None):\n",
        "        assert input.dim() == 2 or input.dim() == 3        \n",
        "        if h0 is None:\n",
        "            h0 = input.data.new(input.size(-2),input.size(-1)).zero_()\n",
        "        IndRNN_Compute = IndRNN_Compute_GPU(self.gradclipvalue)\n",
        "        #h=IndRNN_Compute(input, self.weight_hh, h0)\n",
        "        return IndRNN_Compute(input, self.weight_hh, h0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cupy in /usr/local/lib/python2.7/dist-packages (6.0.0)\n",
            "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python2.7/dist-packages (from cupy) (0.4)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from cupy) (1.16.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from cupy) (1.12.0)\n",
            "Requirement already satisfied: pynvrtc in /usr/local/lib/python2.7/dist-packages (9.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Uz-WO0vu6OI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "import torch.nn.init as weight_init\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "#from cuda_IndRNN_onlyrecurrent import IndRNN_onlyrecurrent as IndRNN\n",
        "#from IndRNN_onlyrecurrent import IndRNN_onlyrecurrent as IndRNN \n",
        "class Batch_norm_step(nn.Module):\n",
        "    def __init__(self,  hidden_size,seq_len):\n",
        "        super(Batch_norm_step, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.max_time_step=seq_len\n",
        "        self.bn = nn.BatchNorm1d(hidden_size) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x=x.permute(1,2,0)\n",
        "        x= self.bn(x.clone())\n",
        "        x=x.permute(2,0,1)\n",
        "        return x\n",
        "class Dropout_overtime(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, input, p=0.5,training=False):\n",
        "    output = input.clone()\n",
        "    noise = input.data.new(input.size(-2),input.size(-1))  #torch.ones_like(input[0])\n",
        "    if training:            \n",
        "      noise.bernoulli_(1 - p).div_(1 - p)\n",
        "      noise = noise.unsqueeze(0).expand_as(input)\n",
        "      output.mul_(noise)\n",
        "    ctx.save_for_backward(noise)\n",
        "    ctx.training=training\n",
        "    return output\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    noise,=ctx.saved_tensors\n",
        "    if ctx.training:\n",
        "      return grad_output.mul(noise),None,None\n",
        "    else:\n",
        "      return grad_output,None,None\n",
        "dropout_overtime=Dropout_overtime.apply\n",
        "\n",
        "#import argparse\n",
        "#import opts     \n",
        "parser = argparse.ArgumentParser(description='pytorch action')\n",
        "#train_opts(parser)\n",
        "#args = parser.parse_args()\n",
        "#MAG=args.MAG\n",
        "U_bound=np.power(10,(np.log10(MAG)/seq_len))\n",
        "U_lowbound=np.power(10,(np.log10(1.0/MAG)/seq_len))  \n",
        "  \n",
        "class stackedIndRNN_encoder(nn.Module):\n",
        "    def __init__(self, input_size, outputclass):\n",
        "        super(stackedIndRNN_encoder, self).__init__()        \n",
        "        #hidden_size=args.hidden_size\n",
        "        \n",
        "        self.DIs=nn.ModuleList()\n",
        "        denseinput=nn.Linear(input_size*n_dimension, hidden_size, bias=True)\n",
        "        self.DIs.append(denseinput)\n",
        "        for x in range(num_layers - 1):\n",
        "            denseinput = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "            self.DIs.append(denseinput)                \n",
        "        \n",
        "        self.BNs = nn.ModuleList()\n",
        "        for x in range(num_layers):\n",
        "            bn = Batch_norm_step(hidden_size,seq_len)\n",
        "            self.BNs.append(bn)                      \n",
        "  \n",
        "        self.RNNs = nn.ModuleList()\n",
        "        rnn = cuda_IndRNN_onlyrecurrent(hidden_size=hidden_size) #IndRNN\n",
        "        self.RNNs.append(rnn)  \n",
        "        for x in range(num_layers-1):\n",
        "            rnn = cuda_IndRNN_onlyrecurrent(hidden_size=hidden_size) #IndRNN\n",
        "            self.RNNs.append(rnn)         \n",
        "            \n",
        "        self.lastfc = nn.Linear(hidden_size, outputclass, bias=True)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "      for name, param in self.named_parameters():\n",
        "        if 'weight_hh' in name:\n",
        "          param.data.uniform_(0,U_bound)          \n",
        "        if 'RNNs.'+str(num_layers-1)+'.weight_hh' in name:\n",
        "          param.data.uniform_(U_lowbound,U_bound)    \n",
        "        if 'DIs' in name and 'weight' in name:\n",
        "          param.data.uniform_(-ini_in2hid,ini_in2hid)               \n",
        "        if 'bns' in name and 'weight' in name:\n",
        "          param.data.fill_(1)      \n",
        "        if 'bias' in name:\n",
        "          param.data.fill_(0.0)              \n",
        "    def forward(self, input):\n",
        "        all_output = []\n",
        "        rnnoutputs={}\n",
        "        hidden_x={}               \n",
        "        seq_len, batch_size, indim,_=input.size()\n",
        "             \n",
        "        input=input.view(seq_len,batch_size,n_dimension*indim)                  \n",
        "        for x in range(1,len(self.RNNs)+1):\n",
        "          hidden_x['hidden%d'%x]=Variable(torch.zeros(1,batch_size,hidden_size).cuda())\n",
        "                            \n",
        "        rnnoutputs['rnnlayer0']=input\n",
        "        for x in range(1,len(self.RNNs)+1):\n",
        "          rnnoutputs['rnnlayer%d'%(x-1)]=rnnoutputs['rnnlayer%d'%(x-1)].view(seq_len*batch_size,-1)\n",
        "          rnnoutputs['rnnlayer%d'%(x-1)]=self.DIs[x-1](rnnoutputs['rnnlayer%d'%(x-1)])   \n",
        "          rnnoutputs['rnnlayer%d'%(x-1)]=rnnoutputs['rnnlayer%d'%(x-1)].view(seq_len,batch_size,-1)  \n",
        "          rnnoutputs['rnnlayer%d'%x]= self.RNNs[x-1](rnnoutputs['rnnlayer%d'%(x-1)], hidden_x['hidden%d'%x])        \n",
        "          rnnoutputs['rnnlayer%d'%x]=self.BNs[x-1](rnnoutputs['rnnlayer%d'%x])     \n",
        "          if dropout>0:\n",
        "            rnnoutputs['rnnlayer%d'%x]= dropout_overtime(rnnoutputs['rnnlayer%d'%x],dropout,self.training) \n",
        "        temp=rnnoutputs['rnnlayer%d'%len(self.RNNs)][-1]\n",
        "        output = self.lastfc(temp)\n",
        "        return output                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMuT59gfnbui",
        "colab_type": "code",
        "outputId": "e8eaf730-299a-4a0c-a89e-5b8b0f13a954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys\n",
        "import h5py\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "#import glob\n",
        "#import skimage.transform\n",
        "#from skimage import color\n",
        "import pickle\n",
        "#import theano\n",
        "#import cv2\n",
        "from multiprocessing import Pool\n",
        "from threading import Thread\n",
        "import os.path\n",
        "#RGB_frames = '/home/sl669/caffe/colordataset/ImageNET/ILSVRC2015/Data/CLS-LOC/val/'#'/home/sl669/caffe/ucf101/framearrays/'#\n",
        "\n",
        "#from __main__ import train_datasets\n",
        "#train_datasets='train_ntus'\n",
        "datasets=train_datasets\n",
        "dataname=datasets+'.npy'\n",
        "labelname=datasets+'_label.npy'\n",
        "lenname=datasets+'_len.npy'\n",
        "data_handle=np.load(dataname, allow_pickle=True)\n",
        "label_handle=np.load(labelname, allow_pickle=True)\n",
        "len_handle=np.load(lenname, allow_pickle=True)\n",
        "num_videos = len(data_handle)  \n",
        "train_no=int(num_videos*0.95)\n",
        "test_no=num_videos-train_no\n",
        "\n",
        "shufflevideolist=np.arange(num_videos)\n",
        "np.random.shuffle(shufflevideolist)\n",
        "\n",
        "shufflevideolist_train=shufflevideolist[:train_no]\n",
        "shufflevideolist_test=shufflevideolist[train_no:]\n",
        "\n",
        "print ('Dataset train size, eval size', train_no,test_no)\n",
        "\n",
        "\n",
        "def rotate( input,s,b):\n",
        "  shape=input.shape\n",
        "  input=input.reshape((-1,3))\n",
        "  XT=input[:,0]\n",
        "  YT=input[:,1]\n",
        "  ZT=input[:,2]\n",
        "  s=s/180.0*np.pi\n",
        "  b=b/180.0*np.pi\n",
        "  RX = XT*np.cos(b) - ZT*np.sin(b) + ZT*np.sin(b)*np.cos(s) + YT*np.sin(b)*np.sin(s) - ZT*np.sin(b)*(np.cos(s) - 1);\n",
        "  RY = YT*np.cos(s);\n",
        "  RZ = ZT*np.cos(b)*np.cos(s) - ZT*(np.cos(b) - 1) - XT*np.sin(b) + YT*np.cos(b)*np.sin(s) - ZT*np.cos(b)*(np.cos(s) - 1);\n",
        "  RX=RX.reshape((-1,1))\n",
        "  RY=RY.reshape((-1,1))\n",
        "  RZ=RZ.reshape((-1,1))\n",
        "  output=np.concatenate([RX,RY,RZ],axis=1)\n",
        "  output=output.reshape(shape)\n",
        "  #print(shape,output.shape,input.shape)\n",
        "  return output \n",
        "\n",
        "class batch_thread_train():\n",
        "  def __init__(self, result, batch_size_,seq_len,use_rotation=False):\n",
        "    self.result = result\n",
        "    self.batch_size_=batch_size_\n",
        "    self.seq_len=seq_len\n",
        "    self.idx=0    \n",
        "    self.use_rotation=use_rotation\n",
        "  \n",
        "  def __call__(self):###Be careful.  The appended data may change like pointer.\n",
        "    templabel=[] \n",
        "    batch_data=[]\n",
        "    for j in range(self.batch_size_):\n",
        "      self.idx +=1\n",
        "      if self.idx == train_no:\n",
        "        self.idx =0\n",
        "        np.random.shuffle(shufflevideolist_train)\n",
        "      shufflevideoindex=shufflevideolist_train[self.idx]\n",
        "      \n",
        "      \n",
        "      label=label_handle[shufflevideoindex]     \n",
        "      templabel.append(np.int32(label))  \n",
        "      dataset=data_handle[shufflevideoindex]\n",
        "      len_data=len_handle[shufflevideoindex] \n",
        "      \n",
        "      dataset = np.asarray(dataset)\n",
        "      \n",
        "      sample=np.zeros(tuple((self.seq_len,)+dataset.shape[1:]))\n",
        "      lenperseg=len_data//self.seq_len\n",
        "      if lenperseg==1 and len_data>self.seq_len:\n",
        "        startid=np.random.randint(len_data-self.seq_len)\n",
        "        sample=dataset[startid:startid+self.seq_len]\n",
        "        #print('wrong data length first')\n",
        "      elif len_data<=self.seq_len:\n",
        "        startid=np.random.randint(max(self.seq_len-len_data,int(0.25*self.seq_len)))\n",
        "        endid=min(self.seq_len,startid+len_data)\n",
        "        datasid=0\n",
        "        dataeid=len_data\n",
        "        if startid+len_data>self.seq_len:\n",
        "          datasid=np.random.randint(startid+len_data-self.seq_len)\n",
        "          dataeid=datasid+self.seq_len-startid\n",
        "        sample[startid:endid]=dataset[datasid:dataeid]\n",
        "      else:      \n",
        "        for framei in range(self.seq_len):        \n",
        "          if framei==self.seq_len-1:\n",
        "            index=lenperseg*framei + np.random.randint(len_data-lenperseg*(self.seq_len-1))\n",
        "          else:\n",
        "            index=lenperseg*framei + np.random.randint(lenperseg)    \n",
        "          sample[framei]=dataset[index]\n",
        "          \n",
        "      \n",
        "      if self.use_rotation:\n",
        "        if np.random.randint(2):\n",
        "          s=np.random.randint(2)*45#random(1)*45\n",
        "          b=np.random.randint(2)*45#random(1)*45\n",
        "          #print(sample.shape)\n",
        "          sample=rotate(sample,s,b)\n",
        "        #print (index,lenperseg)  \n",
        "#       rframei=np.random.randint(len_data)  \n",
        "#       tmean=(dataset[rframei,0,:]+dataset[rframei,12,:]+dataset[rframei,16,:])/3\n",
        "#       sample=sample-tmean  \n",
        "      batch_data.append(sample) ###Be careful. It has to be different. Otherwise, the appended data will change as well.\n",
        "      #print(batch_data)  \n",
        "    \n",
        "      \n",
        "    self.result['data']=np.asarray(batch_data,dtype=np.float32)\n",
        "    self.result['label']= np.asarray(templabel,dtype=np.int32)\n",
        "    \n",
        "\n",
        "class DataHandler_train(object):\n",
        "\n",
        "  def __init__(self, batch_size, seq_len, use_rotation=False):#datasets,\n",
        "    self.batch_size_ = batch_size\t\t\n",
        "    #self.datasets = datasets    \n",
        "    random.seed(10)  \n",
        "    \n",
        "    self.thread_result = {}\n",
        "    self.thread = None\n",
        "\n",
        "    self.batch_advancer =batch_thread_train(self.thread_result,self.batch_size_,seq_len,use_rotation)\n",
        "    \n",
        "    self.dispatch_worker()\n",
        "    self.join_worker()\n",
        "\n",
        "\n",
        "  def GetBatch(self):\n",
        "    #self.batch_data_  = np.zeros((self.batch_size_, 3, self.seq_length_, 112, 112), dtype=np.float32)\n",
        "    if self.thread is not None:\n",
        "      self.join_worker() \n",
        "\n",
        "    self.batch_data_=self.thread_result['data']\n",
        "    self.batch_label_= self.thread_result['label']\n",
        "        \n",
        "    self.dispatch_worker()\n",
        "    return self.batch_data_, self.batch_label_\n",
        "\n",
        "  def dispatch_worker(self):\n",
        "    assert self.thread is None\n",
        "    self.thread = Thread(target=self.batch_advancer)\n",
        "    self.thread.start()\n",
        "\n",
        "  def join_worker(self):\n",
        "    assert self.thread is not None\n",
        "    self.thread.join()\n",
        "    self.thread = None\n",
        "    \n",
        "  def GetDatasetSize(self):\n",
        "    return train_no\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class batch_thread_eval():\n",
        "  def __init__(self, result, batch_size_,seq_len):\n",
        "    self.result = result\n",
        "    self.batch_size_=batch_size_\n",
        "    self.seq_len=seq_len\n",
        "    self.idx=0    \n",
        "  \n",
        "  def __call__(self):###Be careful.  The appended data may change like pointer.\n",
        "    templabel=[] \n",
        "    batch_data=[]\n",
        "    for j in range(self.batch_size_):\n",
        "      self.idx +=1\n",
        "      if self.idx == test_no:\n",
        "        self.idx =0\n",
        "        np.random.shuffle(shufflevideolist_test)\n",
        "      shufflevideoindex=shufflevideolist_test[self.idx]\n",
        "      \n",
        "      \n",
        "      label=label_handle[shufflevideoindex]     \n",
        "      templabel.append(np.int32(label))  \n",
        "      dataset=data_handle[shufflevideoindex]\n",
        "      len_data=len_handle[shufflevideoindex]\n",
        "      \n",
        "      dataset = np.asarray(dataset)\n",
        "      \n",
        "      sample=np.zeros(tuple((self.seq_len,)+dataset.shape[1:]))\n",
        "      lenperseg=len_data//self.seq_len\n",
        "      if lenperseg==1 and len_data>self.seq_len:\n",
        "        startid=np.random.randint(len_data-self.seq_len)\n",
        "        sample=dataset[startid:startid+self.seq_len]\n",
        "      elif len_data<=self.seq_len:\n",
        "        startid=np.random.randint(max(self.seq_len-len_data,int(0.25*self.seq_len)))\n",
        "        endid=min(self.seq_len,startid+len_data)\n",
        "        datasid=0\n",
        "        dataeid=len_data\n",
        "        if startid+len_data>self.seq_len:\n",
        "          datasid=np.random.randint(startid+len_data-self.seq_len)\n",
        "          dataeid=datasid+self.seq_len-startid\n",
        "        sample[startid:endid]=dataset[datasid:dataeid]\n",
        "      else:      \n",
        "        for framei in range(self.seq_len):        \n",
        "          if framei==self.seq_len-1:\n",
        "            index=lenperseg*framei + np.random.randint(len_data-lenperseg*(self.seq_len-1))\n",
        "          else:\n",
        "            index=lenperseg*framei + np.random.randint(lenperseg)    \n",
        "          sample[framei]=dataset[index]\n",
        "        #print (index,lenperseg)  \n",
        "        \n",
        "      batch_data.append(sample) ###Be careful. It has to be different. Otherwise, the appended data will change as well.\n",
        "      #print(batch_data)       \n",
        "      \n",
        "    self.result['data']=np.asarray(batch_data,dtype=np.float32)\n",
        "    self.result['label']= np.asarray(templabel,dtype=np.int32)   \n",
        "\n",
        "class DataHandler_eval(object):\n",
        "\n",
        "  def __init__(self, batch_size, seq_len):#, datasets\n",
        "    self.batch_size_ = batch_size    \n",
        "    #self.datasets = datasets    \n",
        "    random.seed(10)  \n",
        "    \n",
        "    self.thread_result = {}\n",
        "    self.thread = None\n",
        "\n",
        "    self.batch_advancer =batch_thread_eval(self.thread_result,self.batch_size_,seq_len)\n",
        "    \n",
        "    self.dispatch_worker()\n",
        "    self.join_worker()\n",
        "\n",
        "\n",
        "  def GetBatch(self):\n",
        "    #self.batch_data_  = np.zeros((self.batch_size_, 3, self.seq_length_, 112, 112), dtype=np.float32)\n",
        "    if self.thread is not None:\n",
        "      self.join_worker() \n",
        "\n",
        "    self.batch_data_=self.thread_result['data']\n",
        "    self.batch_label_= self.thread_result['label']\n",
        "        \n",
        "    self.dispatch_worker()\n",
        "    return self.batch_data_, self.batch_label_\n",
        "\n",
        "  def dispatch_worker(self):\n",
        "    assert self.thread is None\n",
        "    self.thread = Thread(target=self.batch_advancer)\n",
        "    self.thread.start()\n",
        "\n",
        "  def join_worker(self):\n",
        "    assert self.thread is not None\n",
        "    self.thread.join()\n",
        "    self.thread = None\n",
        "    \n",
        "  def GetDatasetSize(self):\n",
        "    return test_no\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset train size, eval size 18752 987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsPN48aPn-_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import h5py\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "#import glob\n",
        "#import skimage.transform\n",
        "#from skimage import color\n",
        "import pickle\n",
        "#import theano\n",
        "#import cv2\n",
        "from multiprocessing import Pool\n",
        "from threading import Thread\n",
        "import os.path\n",
        "#RGB_frames = '/home/sl669/caffe/colordataset/ImageNET/ILSVRC2015/Data/CLS-LOC/val/'#'/home/sl669/caffe/ucf101/framearrays/'#\n",
        "\n",
        "\n",
        "\n",
        "#from __main__ import test_dataset\n",
        "datasets=test_dataset\n",
        "class batch_thread():\n",
        "  def __init__(self, result, batch_size_,seq_len):#, datasets\n",
        "    self.result = result\n",
        "    self.batch_size_=batch_size_\n",
        "    self.datasets = datasets   \n",
        "    self.seq_len=seq_len\n",
        "    self.idx=-1\n",
        "    \n",
        "    dataname=datasets+'.npy'\n",
        "    labelname=datasets+'_label.npy'\n",
        "    lenname=datasets+'_len.npy'\n",
        "    self.data_handle=np.load(dataname, allow_pickle=True)\n",
        "    self.label_handle=np.load(labelname, allow_pickle=True)\n",
        "    self.len_handle=np.load(lenname, allow_pickle=True) \n",
        "    \n",
        "    self.num_videos = len(self.data_handle)    \n",
        "    self.shufflevideolist=np.arange(self.num_videos)\n",
        "    np.random.shuffle(self.shufflevideolist)\n",
        "\n",
        "    print ('Dataset test size', self.num_videos)\n",
        "  \n",
        "  def __call__(self):###Be careful.  The appended data may change like pointer.\n",
        "    templabel=[] \n",
        "    batch_data=[]\n",
        "    tempindex=[] \n",
        "    for j in range(self.batch_size_):\n",
        "      self.idx +=1\n",
        "      if self.idx == self.num_videos:\n",
        "        self.idx =0\n",
        "        np.random.shuffle(self.shufflevideolist)\n",
        "      shufflevideoindex=self.shufflevideolist[self.idx]\n",
        "      \n",
        "      label=self.label_handle[shufflevideoindex]     \n",
        "      templabel.append(np.int32(label))  \n",
        "      tempindex.append(np.int32(shufflevideoindex)) \n",
        "      dataset=self.data_handle[shufflevideoindex]\n",
        "      len_data=self.len_handle[shufflevideoindex] \n",
        "      \n",
        "      dataset = np.asarray(dataset)\n",
        "      \n",
        "      sample=np.zeros(tuple((self.seq_len,)+dataset.shape[1:]))\n",
        "      lenperseg=len_data//self.seq_len\n",
        "      if lenperseg==1 and len_data>self.seq_len:\n",
        "        startid=np.random.randint(len_data-self.seq_len)\n",
        "        sample=dataset[startid:startid+self.seq_len]\n",
        "      elif len_data<=self.seq_len:\n",
        "        startid=np.random.randint(max(self.seq_len-len_data,int(0.25*self.seq_len)))\n",
        "        endid=min(self.seq_len,startid+len_data)\n",
        "        datasid=0\n",
        "        dataeid=len_data\n",
        "        if startid+len_data>self.seq_len:\n",
        "          datasid=np.random.randint(startid+len_data-self.seq_len)\n",
        "          dataeid=datasid+self.seq_len-startid\n",
        "        sample[startid:endid]=dataset[datasid:dataeid]\n",
        "      else:      \n",
        "        for framei in range(self.seq_len):        \n",
        "          if framei==self.seq_len-1:\n",
        "            index=lenperseg*framei + np.random.randint(len_data-lenperseg*(self.seq_len-1))\n",
        "          else:\n",
        "            index=lenperseg*framei + np.random.randint(lenperseg)    \n",
        "          sample[framei]=dataset[index]\n",
        "        #print (index,lenperseg)  \n",
        "        \n",
        "      batch_data.append(sample) ###Be careful. It has to be different. Otherwise, the appended data will change as well.\n",
        "      #print(batch_data)       \n",
        "\n",
        "       \n",
        "    self.result['data']=np.asarray(batch_data,dtype=np.float32)\n",
        "    self.result['label']= np.asarray(templabel,dtype=np.int32)   \n",
        "    self.result['index']= np.asarray(tempindex,dtype=np.int32)   \n",
        "      \n",
        "      \n",
        "  def GetDatasetSize(self):\n",
        "    return self.num_videos\n",
        "\n",
        "\n",
        "\n",
        "class testDataHandler(object):\n",
        "\n",
        "  def __init__(self, batch_size, seq_len):#, datasets\n",
        "    self.batch_size_ = batch_size\t\t\n",
        "    #self.datasets = datasets    \n",
        "    random.seed(10)  \n",
        "    \n",
        "    self.thread_result = {}\n",
        "    self.thread = None\n",
        "\n",
        "    self.batch_advancer =batch_thread(self.thread_result,self.batch_size_,seq_len)#, self.datasets\n",
        "    \n",
        "    self.datasetsize=self.batch_advancer.GetDatasetSize()\n",
        "    \n",
        "    self.dispatch_worker()\n",
        "    self.join_worker()\n",
        "\n",
        "\n",
        "  def GetBatch(self):\n",
        "    #self.batch_data_  = np.zeros((self.batch_size_, 3, self.seq_length_, 112, 112), dtype=np.float32)\n",
        "    if self.thread is not None:\n",
        "      self.join_worker() \n",
        "      \n",
        "#     self.batch_data_=self.thread_result['data']\n",
        "#     self.batch_label_=self.thread_result['label']\n",
        "\n",
        "    self.batch_data_=self.thread_result['data']\n",
        "    self.batch_label_= self.thread_result['label']\n",
        "    self.batch_index_= self.thread_result['index']\n",
        "        \n",
        "    self.dispatch_worker()\n",
        "    return self.batch_data_, self.batch_label_,self.batch_index_\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  def dispatch_worker(self):\n",
        "    assert self.thread is None\n",
        "    self.thread = Thread(target=self.batch_advancer)\n",
        "    self.thread.start()\n",
        "\n",
        "  def join_worker(self):\n",
        "    assert self.thread is not None\n",
        "    self.thread.join()\n",
        "    self.thread = None\n",
        "    \n",
        "  def GetDatasetSize(self):\n",
        "    return self.datasetsize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZODbcHZ1W42i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def get_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    label_exist = [0]*len(classes)\n",
        "    missing_labels = []\n",
        "    for l in y_pred:\n",
        "      label_exist[l]=1\n",
        "    for l in y_true:\n",
        "      label_exist[l]=1\n",
        "    for i in range(len(classes)):\n",
        "      if label_exist[i] is 0:\n",
        "        if i not in missing_labels:\n",
        "          missing_labels.append(i)\n",
        "      \n",
        "    missing_labels.sort()\n",
        "    for l in missing_labels:\n",
        "      cm = np.insert(cm,l,0,axis=0)\n",
        "      cm = np.insert(cm,l,0,axis=1)\n",
        "    classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        pass\n",
        "        #print('Confusion matrix, without normalization')\n",
        "\n",
        "    return cm\n",
        "    \n",
        "\n",
        "def plot_confusion_matrix(cm, classes, cmap=plt.cm.Blues, title=None, \n",
        "                          normalize=False):\n",
        "  \n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2vkp4NwvGM1",
        "colab_type": "code",
        "outputId": "4e980f16-cdd7-49e2-ec8b-70b04a87406f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18180
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "#import argparse\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "seed=100\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  pass\n",
        "else:\n",
        "  print(\"WARNING: CUDA not available\")\n",
        "\n",
        "#import opts     \n",
        "#parser = argparse.ArgumentParser(description='pytorch action')\n",
        "#opts.train_opts(parser)\n",
        "#args = parser.parse_args()\n",
        "#print(args)\n",
        "\n",
        "#import Indrnn_action_network\n",
        "\n",
        "#batch_size = args.batch_size\n",
        "#seq_len=args.seq_len\n",
        "in_size=num_joints\n",
        "gradientclip_value=10\n",
        "#U_bound=Indrnn_action_network.U_bound\n",
        "\n",
        "\n",
        "\n",
        "model = stackedIndRNN_encoder(in_size, outputclass)  \n",
        "model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Adam with lr 2e-4 works fine.\n",
        "learning_rate=lr\n",
        "if use_weightdecay_nohiddenW:\n",
        "  param_decay=[]\n",
        "  param_nodecay=[]\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'weight_hh' in name or 'bias' in name:\n",
        "      param_nodecay.append(param)      \n",
        "      #print('parameters no weight decay: ',name)          \n",
        "    else:\n",
        "      param_decay.append(param)      \n",
        "      #print('parameters with weight decay: ',name)          \n",
        "\n",
        "  if opti=='sgd':\n",
        "    optimizer = torch.optim.SGD([\n",
        "            {'params': param_nodecay},\n",
        "            {'params': param_decay, 'weight_decay': decayfactor}\n",
        "        ], lr=learning_rate,momentum=0.9,nesterov=True)   \n",
        "  else:                \n",
        "    optimizer = torch.optim.Adam([\n",
        "            {'params': param_nodecay},\n",
        "            {'params': param_decay, 'weight_decay': decayfactor}\n",
        "        ], lr=learning_rate) \n",
        "else:  \n",
        "  if opti=='sgd':   \n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate,momentum=0.9,nesterov=True)\n",
        "  else:                      \n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  \n",
        "#from data_reader_numpy_witheval import DataHandler_train,DataHandler_eval  \n",
        "#from data_reader_numpy_test import DataHandler as testDataHandler\n",
        "dh_train = DataHandler_train(batch_size,seq_len)\n",
        "dh_eval = DataHandler_eval(batch_size,seq_len)\n",
        "dh_test= testDataHandler(batch_size,seq_len)\n",
        "num_train_batches=int(np.ceil(dh_train.GetDatasetSize()/(batch_size+0.0)))\n",
        "num_eval_batches=int(np.ceil(dh_eval.GetDatasetSize()/(batch_size+0.0)))\n",
        "num_test_batches=int(np.ceil(dh_test.GetDatasetSize()/(batch_size+0.0)))\n",
        "#print(num_train_batches)\n",
        "\n",
        "\n",
        "def train(num_train_batches):\n",
        "  model.train()\n",
        "  tacc=0\n",
        "  count=0\n",
        "  start_time = time.time()\n",
        "  for batchi in range(0,num_train_batches):\n",
        "    inputs,targets=dh_train.GetBatch()\n",
        "    #print(inputs.shape)\n",
        "    inputs=inputs.transpose(1,0,2,3)\n",
        "    \n",
        "    \n",
        "    inputs=Variable(torch.from_numpy(inputs).cuda(), requires_grad=True)\n",
        "    targets=Variable(torch.from_numpy(np.int64(targets)).cuda(), requires_grad=False)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if constrain_U:\n",
        "      clip_weight(model,U_bound)\n",
        "    output=model(inputs)\n",
        "    loss = criterion(output, targets)\n",
        "\n",
        "    pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "    accuracy = pred.eq(targets.data).cpu().sum().numpy()/(0.0+targets.size(0))      \n",
        "          \n",
        "    loss.backward()\n",
        "    clip_gradient(model,gradientclip_value)\n",
        "    optimizer.step()\n",
        "    \n",
        "    tacc=tacc+accuracy#loss.data.cpu().numpy()#accuracy\n",
        "    count+=1\n",
        "  elapsed = time.time() - start_time\n",
        "  print (\"training accuracy: \", tacc/(count+0.0)  )\n",
        "  #print ('time per batch: ', elapsed/num_train_batches)\n",
        "  #print ('time per epoch: ', elapsed)\n",
        "  \n",
        "def set_bn_train(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('BatchNorm') != -1:\n",
        "      m.train()       \n",
        "def eval(dh,num_batches,use_bn_trainstat=False):\n",
        "  model.eval()\n",
        "  if use_bn_trainstat:\n",
        "    model.apply(set_bn_train)\n",
        "  tacc=0\n",
        "  count=0  \n",
        "  start_time = time.time()\n",
        "  while(1):  \n",
        "    inputs,targets=dh.GetBatch()\n",
        "    inputs=inputs.transpose(1,0,2,3)\n",
        "    inputs=Variable(torch.from_numpy(inputs).cuda())\n",
        "    targets=Variable(torch.from_numpy(np.int64(targets)).cuda())\n",
        "        \n",
        "    output=model(inputs)\n",
        "    pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "    accuracy = pred.eq(targets.data).cpu().sum().numpy()        \n",
        "    tacc+=accuracy\n",
        "    count+=1\n",
        "    if count==num_batches*eval_fold:\n",
        "      break\n",
        "  elapsed = time.time() - start_time\n",
        "  print (\"eval accuracy: \", tacc/(count*targets.data.size(0)+0.0)  )\n",
        "  #print ('eval time per batch: ', elapsed/(count+0.0))\n",
        "  return tacc/(count*targets.data.size(0)+0.0)\n",
        "\n",
        "\n",
        "def test(dh,num_batches,use_bn_trainstat=False, save_cm=False):\n",
        "  model.eval()\n",
        "  if use_bn_trainstat:\n",
        "    model.apply(set_bn_train)\n",
        "  tacc=0\n",
        "  count=0  \n",
        "  start_time = time.time()\n",
        "  total_testdata=dh.GetDatasetSize()  \n",
        "  total_ave_acc=np.zeros((total_testdata,outputclass))\n",
        "  testlabels=np.zeros((total_testdata))\n",
        "  #print(\"number of tests: \", test_no)\n",
        "  cm = np.zeros((outputclass,outputclass)).astype(int)\n",
        "  if outputclass is 2:\n",
        "    class_names = np.array(['non-fall', 'fall'])\n",
        "  else:\n",
        "    class_names = np.array(range(60))\n",
        "  while(1):  \n",
        "    inputs,targets,index=dh.GetBatch()\n",
        "    inputs=inputs.transpose(1,0,2,3)\n",
        "    testlabels[index]=targets\n",
        "    inputs=Variable(torch.from_numpy(inputs).cuda())\n",
        "    targets=Variable(torch.from_numpy(np.int64(targets)).cuda())\n",
        "        \n",
        "    output=model(inputs)\n",
        "    pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "    accuracy = pred.eq(targets.data).cpu().sum().numpy()    \n",
        "    total_ave_acc[index]+=output.data.cpu().numpy()\n",
        "\n",
        "    # Plot non-normalized confusion matrix\n",
        "    cm += get_confusion_matrix(targets.cpu().data.cpu().numpy().astype(int), pred.cpu().data.cpu().numpy().astype(int), classes=class_names)\n",
        "    \n",
        "    tacc+=accuracy\n",
        "    count+=1\n",
        "    if count==global_test_no*num_batches:\n",
        "      break    \n",
        "  #total_ave_acc/=args.test_no\n",
        "  np.set_printoptions(precision=2)\n",
        "  plot_confusion_matrix(cm, classes=class_names, title='Confusion matrix, without normalization')\n",
        "  plt.show()\n",
        "  \n",
        "  if save_cm:\n",
        "    np.save('cm.npy',cm)\n",
        "    model_file = drive.CreateFile({'title' : 'cm.npy'})\n",
        "    model_file.SetContentFile('cm.npy')\n",
        "    model_file.Upload()\n",
        "\n",
        "    # download to google drive\n",
        "    drive.CreateFile({'id': model_file.get('id')})\n",
        "\n",
        "\n",
        "  top = np.argmax(total_ave_acc, axis=-1)\n",
        "  eval_acc=np.mean(np.equal(top, testlabels))    \n",
        "  elapsed = time.time() - start_time\n",
        "  print (\"test accuracy: \", tacc/(count*targets.data.size(0)+0.0), \"eval accuracy: \", eval_acc, \"( use_bn_trainstat=\", use_bn_trainstat, \")\"  )\n",
        "  #print ('test time per batch: ', elapsed/(count+0.0))\n",
        "  \n",
        "  \n",
        "  return tacc/(count*targets.data.size(0)+0.0)#, eval_acc/(total_testdata+0.0)\n",
        "\n",
        "def clip_gradient(model, clip):\n",
        "    for p in model.parameters():\n",
        "        p.grad.data.clamp_(-clip,clip)\n",
        "        #print(p.size(),p.grad.data)\n",
        "\n",
        "def adjust_learning_rate(optimizer, lr):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr     \n",
        "\n",
        "def clip_weight(RNNmodel, clip):\n",
        "    for name, param in RNNmodel.named_parameters():\n",
        "      if 'weight_hh' in name:\n",
        "        param.data.clamp_(-clip,clip)\n",
        "    \n",
        "lastacc=0\n",
        "eval_dispFreq=20\n",
        "patience=0\n",
        "reduced=1\n",
        "for i in range(1,301):\n",
        "  print(\"Epoch: \", i)\n",
        "  for _ in range(num_train_batches//eval_dispFreq):\n",
        "    train(eval_dispFreq)\n",
        "  test_acc=eval(dh_eval,num_eval_batches,use_bneval)\n",
        "\n",
        "  model_clone = copy.deepcopy(model.state_dict())\n",
        "  opti_clone = copy.deepcopy(optimizer.state_dict())\n",
        "  if (test_acc >lastacc):  \n",
        "    lastacc=test_acc\n",
        "    patience=0\n",
        "  elif patience>int(pThre/reduced+0.5):\n",
        "    reduced=reduced*2\n",
        "    print ('learning rate',learning_rate)\n",
        "    model.load_state_dict(model_clone)\n",
        "    optimizer.load_state_dict(opti_clone)\n",
        "    patience=0\n",
        "    learning_rate=learning_rate*0.1\n",
        "    adjust_learning_rate(optimizer,learning_rate)     \n",
        "    if learning_rate<end_rate:\n",
        "      break  \n",
        "    test_acc=test(dh_test,num_test_batches)     \n",
        " \n",
        "  else:\n",
        "    patience+=1 \n",
        "  print('\\n')\n",
        "    \n",
        "test_acc=test(dh_test,num_test_batches)  \n",
        "#test_acc=test(dh_test,num_test_batches,True)   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:224: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset test size 2575\n",
            "Epoch:  1\n",
            "IndRNN loaded for gpu 0\n",
            "training accuracy:  0.649609375\n",
            "training accuracy:  0.733203125\n",
            "training accuracy:  0.790625\n",
            "training accuracy:  0.80390625\n",
            "training accuracy:  0.833984375\n",
            "training accuracy:  0.860546875\n",
            "training accuracy:  0.871484375\n",
            "eval accuracy:  0.8923828125\n",
            "\n",
            "\n",
            "Epoch:  2\n",
            "training accuracy:  0.891796875\n",
            "training accuracy:  0.907421875\n",
            "training accuracy:  0.90546875\n",
            "training accuracy:  0.909765625\n",
            "training accuracy:  0.92578125\n",
            "training accuracy:  0.91953125\n",
            "training accuracy:  0.928515625\n",
            "eval accuracy:  0.948046875\n",
            "\n",
            "\n",
            "Epoch:  3\n",
            "training accuracy:  0.93046875\n",
            "training accuracy:  0.934375\n",
            "training accuracy:  0.94140625\n",
            "training accuracy:  0.939453125\n",
            "training accuracy:  0.943359375\n",
            "training accuracy:  0.94765625\n",
            "training accuracy:  0.929296875\n",
            "eval accuracy:  0.93984375\n",
            "\n",
            "\n",
            "Epoch:  4\n",
            "training accuracy:  0.94453125\n",
            "training accuracy:  0.947265625\n",
            "training accuracy:  0.95234375\n",
            "training accuracy:  0.941015625\n",
            "training accuracy:  0.94609375\n",
            "training accuracy:  0.955859375\n",
            "training accuracy:  0.95703125\n",
            "eval accuracy:  0.9736328125\n",
            "\n",
            "\n",
            "Epoch:  5\n",
            "training accuracy:  0.948828125\n",
            "training accuracy:  0.957421875\n",
            "training accuracy:  0.9625\n",
            "training accuracy:  0.959765625\n",
            "training accuracy:  0.971875\n",
            "training accuracy:  0.9609375\n",
            "training accuracy:  0.958984375\n",
            "eval accuracy:  0.965625\n",
            "\n",
            "\n",
            "Epoch:  6\n",
            "training accuracy:  0.9515625\n",
            "training accuracy:  0.96015625\n",
            "training accuracy:  0.958203125\n",
            "training accuracy:  0.958203125\n",
            "training accuracy:  0.966796875\n",
            "training accuracy:  0.96953125\n",
            "training accuracy:  0.9703125\n",
            "eval accuracy:  0.975\n",
            "\n",
            "\n",
            "Epoch:  7\n",
            "training accuracy:  0.966015625\n",
            "training accuracy:  0.969921875\n",
            "training accuracy:  0.97265625\n",
            "training accuracy:  0.975\n",
            "training accuracy:  0.9765625\n",
            "training accuracy:  0.971484375\n",
            "training accuracy:  0.96875\n",
            "eval accuracy:  0.98671875\n",
            "\n",
            "\n",
            "Epoch:  8\n",
            "training accuracy:  0.965625\n",
            "training accuracy:  0.965234375\n",
            "training accuracy:  0.965234375\n",
            "training accuracy:  0.969140625\n",
            "training accuracy:  0.976953125\n",
            "training accuracy:  0.98203125\n",
            "training accuracy:  0.975\n",
            "eval accuracy:  0.987890625\n",
            "\n",
            "\n",
            "Epoch:  9\n",
            "training accuracy:  0.978515625\n",
            "training accuracy:  0.973046875\n",
            "training accuracy:  0.969140625\n",
            "training accuracy:  0.976171875\n",
            "training accuracy:  0.977734375\n",
            "training accuracy:  0.974609375\n",
            "training accuracy:  0.977734375\n",
            "eval accuracy:  0.9880859375\n",
            "\n",
            "\n",
            "Epoch:  10\n",
            "training accuracy:  0.973828125\n",
            "training accuracy:  0.9734375\n",
            "training accuracy:  0.977734375\n",
            "training accuracy:  0.975390625\n",
            "training accuracy:  0.978125\n",
            "training accuracy:  0.9734375\n",
            "training accuracy:  0.976953125\n",
            "eval accuracy:  0.9873046875\n",
            "\n",
            "\n",
            "Epoch:  11\n",
            "training accuracy:  0.974609375\n",
            "training accuracy:  0.978515625\n",
            "training accuracy:  0.97578125\n",
            "training accuracy:  0.978515625\n",
            "training accuracy:  0.98125\n",
            "training accuracy:  0.977734375\n",
            "training accuracy:  0.97109375\n",
            "eval accuracy:  0.986328125\n",
            "\n",
            "\n",
            "Epoch:  12\n",
            "training accuracy:  0.980859375\n",
            "training accuracy:  0.981640625\n",
            "training accuracy:  0.972265625\n",
            "training accuracy:  0.98359375\n",
            "training accuracy:  0.978515625\n",
            "training accuracy:  0.984765625\n",
            "training accuracy:  0.983984375\n",
            "eval accuracy:  0.9896484375\n",
            "\n",
            "\n",
            "Epoch:  13\n",
            "training accuracy:  0.983984375\n",
            "training accuracy:  0.98125\n",
            "training accuracy:  0.98046875\n",
            "training accuracy:  0.979296875\n",
            "training accuracy:  0.9875\n",
            "training accuracy:  0.9828125\n",
            "training accuracy:  0.980078125\n",
            "eval accuracy:  0.992578125\n",
            "\n",
            "\n",
            "Epoch:  14\n",
            "training accuracy:  0.97890625\n",
            "training accuracy:  0.977734375\n",
            "training accuracy:  0.978125\n",
            "training accuracy:  0.980078125\n",
            "training accuracy:  0.983984375\n",
            "training accuracy:  0.9859375\n",
            "training accuracy:  0.984765625\n",
            "eval accuracy:  0.99453125\n",
            "\n",
            "\n",
            "Epoch:  15\n",
            "training accuracy:  0.98203125\n",
            "training accuracy:  0.983984375\n",
            "training accuracy:  0.986328125\n",
            "training accuracy:  0.980859375\n",
            "training accuracy:  0.98046875\n",
            "training accuracy:  0.97734375\n",
            "training accuracy:  0.98671875\n",
            "eval accuracy:  0.983984375\n",
            "\n",
            "\n",
            "Epoch:  16\n",
            "training accuracy:  0.984375\n",
            "training accuracy:  0.980859375\n",
            "training accuracy:  0.97890625\n",
            "training accuracy:  0.981640625\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.983984375\n",
            "eval accuracy:  0.99296875\n",
            "\n",
            "\n",
            "Epoch:  17\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.985546875\n",
            "training accuracy:  0.982421875\n",
            "training accuracy:  0.97890625\n",
            "training accuracy:  0.9828125\n",
            "training accuracy:  0.987109375\n",
            "training accuracy:  0.984375\n",
            "eval accuracy:  0.99609375\n",
            "\n",
            "\n",
            "Epoch:  18\n",
            "training accuracy:  0.9796875\n",
            "training accuracy:  0.983203125\n",
            "training accuracy:  0.979296875\n",
            "training accuracy:  0.98671875\n",
            "training accuracy:  0.982421875\n",
            "training accuracy:  0.9875\n",
            "training accuracy:  0.980859375\n",
            "eval accuracy:  0.9923828125\n",
            "\n",
            "\n",
            "Epoch:  19\n",
            "training accuracy:  0.97890625\n",
            "training accuracy:  0.984375\n",
            "training accuracy:  0.981640625\n",
            "training accuracy:  0.980078125\n",
            "training accuracy:  0.98671875\n",
            "training accuracy:  0.9859375\n",
            "training accuracy:  0.9828125\n",
            "eval accuracy:  0.9892578125\n",
            "\n",
            "\n",
            "Epoch:  20\n",
            "training accuracy:  0.98125\n",
            "training accuracy:  0.985546875\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.98125\n",
            "training accuracy:  0.986328125\n",
            "training accuracy:  0.988671875\n",
            "eval accuracy:  0.9939453125\n",
            "\n",
            "\n",
            "Epoch:  21\n",
            "training accuracy:  0.983984375\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.982421875\n",
            "training accuracy:  0.98671875\n",
            "training accuracy:  0.981640625\n",
            "eval accuracy:  0.991796875\n",
            "\n",
            "\n",
            "Epoch:  22\n",
            "training accuracy:  0.9859375\n",
            "training accuracy:  0.98203125\n",
            "training accuracy:  0.985546875\n",
            "training accuracy:  0.984765625\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.98515625\n",
            "training accuracy:  0.9828125\n",
            "eval accuracy:  0.9955078125\n",
            "\n",
            "\n",
            "Epoch:  23\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.983203125\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.987109375\n",
            "training accuracy:  0.987109375\n",
            "training accuracy:  0.9859375\n",
            "training accuracy:  0.9828125\n",
            "eval accuracy:  0.983984375\n",
            "\n",
            "\n",
            "Epoch:  24\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.986328125\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.98515625\n",
            "training accuracy:  0.984375\n",
            "eval accuracy:  0.9939453125\n",
            "\n",
            "\n",
            "Epoch:  25\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.98671875\n",
            "training accuracy:  0.984375\n",
            "training accuracy:  0.9875\n",
            "training accuracy:  0.9859375\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.98984375\n",
            "eval accuracy:  0.994921875\n",
            "\n",
            "\n",
            "Epoch:  26\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.985546875\n",
            "training accuracy:  0.986328125\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.988671875\n",
            "eval accuracy:  0.9958984375\n",
            "\n",
            "\n",
            "Epoch:  27\n",
            "training accuracy:  0.987109375\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.98671875\n",
            "training accuracy:  0.984765625\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.987109375\n",
            "training accuracy:  0.989453125\n",
            "eval accuracy:  0.9919921875\n",
            "\n",
            "\n",
            "Epoch:  28\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.987109375\n",
            "training accuracy:  0.984765625\n",
            "training accuracy:  0.984375\n",
            "training accuracy:  0.9859375\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.989453125\n",
            "eval accuracy:  0.994921875\n",
            "\n",
            "\n",
            "Epoch:  29\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.991796875\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.9875\n",
            "training accuracy:  0.9859375\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.984765625\n",
            "eval accuracy:  0.9947265625\n",
            "\n",
            "\n",
            "Epoch:  30\n",
            "training accuracy:  0.987109375\n",
            "training accuracy:  0.991796875\n",
            "training accuracy:  0.9875\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.984765625\n",
            "eval accuracy:  0.9939453125\n",
            "\n",
            "\n",
            "Epoch:  31\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.98671875\n",
            "training accuracy:  0.985546875\n",
            "training accuracy:  0.9875\n",
            "training accuracy:  0.985546875\n",
            "training accuracy:  0.986328125\n",
            "eval accuracy:  0.9947265625\n",
            "\n",
            "\n",
            "Epoch:  32\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.985546875\n",
            "training accuracy:  0.9875\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.989453125\n",
            "eval accuracy:  0.9919921875\n",
            "\n",
            "\n",
            "Epoch:  33\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.9875\n",
            "training accuracy:  0.98671875\n",
            "eval accuracy:  0.9962890625\n",
            "\n",
            "\n",
            "Epoch:  34\n",
            "training accuracy:  0.98359375\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.991796875\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.9875\n",
            "training accuracy:  0.984375\n",
            "eval accuracy:  0.9943359375\n",
            "\n",
            "\n",
            "Epoch:  35\n",
            "training accuracy:  0.99140625\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.98671875\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.985546875\n",
            "training accuracy:  0.991796875\n",
            "eval accuracy:  0.996875\n",
            "\n",
            "\n",
            "Epoch:  36\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.991796875\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.993359375\n",
            "eval accuracy:  0.996484375\n",
            "\n",
            "\n",
            "Epoch:  37\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.98671875\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.987890625\n",
            "eval accuracy:  0.9970703125\n",
            "\n",
            "\n",
            "Epoch:  38\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.98671875\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.9875\n",
            "eval accuracy:  0.9947265625\n",
            "\n",
            "\n",
            "Epoch:  39\n",
            "training accuracy:  0.984765625\n",
            "training accuracy:  0.987109375\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.9875\n",
            "training accuracy:  0.987109375\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.991015625\n",
            "eval accuracy:  0.994921875\n",
            "\n",
            "\n",
            "Epoch:  40\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.99140625\n",
            "training accuracy:  0.99140625\n",
            "training accuracy:  0.992578125\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.990625\n",
            "eval accuracy:  0.9984375\n",
            "\n",
            "\n",
            "Epoch:  41\n",
            "training accuracy:  0.992578125\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.99140625\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.990625\n",
            "eval accuracy:  0.9962890625\n",
            "\n",
            "\n",
            "Epoch:  42\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.992578125\n",
            "eval accuracy:  0.9966796875\n",
            "\n",
            "\n",
            "Epoch:  43\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.992578125\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.9875\n",
            "eval accuracy:  0.9927734375\n",
            "\n",
            "\n",
            "Epoch:  44\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.991796875\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.992578125\n",
            "eval accuracy:  0.99453125\n",
            "\n",
            "\n",
            "Epoch:  45\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.99375\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.992578125\n",
            "eval accuracy:  0.9962890625\n",
            "\n",
            "\n",
            "Epoch:  46\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.991796875\n",
            "training accuracy:  0.993359375\n",
            "eval accuracy:  0.9962890625\n",
            "\n",
            "\n",
            "Epoch:  47\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.99140625\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.989453125\n",
            "eval accuracy:  0.99765625\n",
            "\n",
            "\n",
            "Epoch:  48\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.994921875\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.992578125\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.99140625\n",
            "eval accuracy:  0.9947265625\n",
            "\n",
            "\n",
            "Epoch:  49\n",
            "training accuracy:  0.98984375\n",
            "training accuracy:  0.991796875\n",
            "training accuracy:  0.99140625\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.9921875\n",
            "eval accuracy:  0.994921875\n",
            "\n",
            "\n",
            "Epoch:  50\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.991796875\n",
            "training accuracy:  0.991796875\n",
            "eval accuracy:  0.995703125\n",
            "\n",
            "\n",
            "Epoch:  51\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.988671875\n",
            "training accuracy:  0.9859375\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.99453125\n",
            "eval accuracy:  0.9966796875\n",
            "\n",
            "\n",
            "Epoch:  52\n",
            "training accuracy:  0.992578125\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.989453125\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.991796875\n",
            "eval accuracy:  0.995703125\n",
            "\n",
            "\n",
            "Epoch:  53\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.99140625\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.991796875\n",
            "eval accuracy:  0.9955078125\n",
            "\n",
            "\n",
            "Epoch:  54\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.98671875\n",
            "training accuracy:  0.98828125\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.991796875\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.99375\n",
            "eval accuracy:  0.9982421875\n",
            "\n",
            "\n",
            "Epoch:  55\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.990234375\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.994921875\n",
            "eval accuracy:  0.997265625\n",
            "\n",
            "\n",
            "Epoch:  56\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.99140625\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.99375\n",
            "training accuracy:  0.993359375\n",
            "eval accuracy:  0.9982421875\n",
            "\n",
            "\n",
            "Epoch:  57\n",
            "training accuracy:  0.991796875\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.99140625\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.994140625\n",
            "eval accuracy:  0.996484375\n",
            "\n",
            "\n",
            "Epoch:  58\n",
            "training accuracy:  0.992578125\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.987890625\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.99375\n",
            "eval accuracy:  0.99765625\n",
            "\n",
            "\n",
            "Epoch:  59\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.991015625\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.993359375\n",
            "eval accuracy:  0.996875\n",
            "\n",
            "\n",
            "Epoch:  60\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.994921875\n",
            "training accuracy:  0.990625\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.99375\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.9921875\n",
            "eval accuracy:  0.9984375\n",
            "\n",
            "\n",
            "Epoch:  61\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.992578125\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.9890625\n",
            "training accuracy:  0.99375\n",
            "training accuracy:  0.994140625\n",
            "eval accuracy:  0.99765625\n",
            "\n",
            "\n",
            "Epoch:  62\n",
            "training accuracy:  0.994921875\n",
            "training accuracy:  0.992578125\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.992578125\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.990625\n",
            "eval accuracy:  0.9982421875\n",
            "learning rate 0.0002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEYCAYAAAA6b7/5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMyvAidLMrAAnSjOzAv4f1g93vHxEPJ0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy:  0.9802827380952381 eval accuracy:  0.9798058252427184 ( use_bn_trainstat= False )\n",
            "\n",
            "\n",
            "Epoch:  63\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.9953125\n",
            "eval accuracy:  0.9982421875\n",
            "\n",
            "\n",
            "Epoch:  64\n",
            "training accuracy:  0.994921875\n",
            "training accuracy:  0.994921875\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.995703125\n",
            "eval accuracy:  0.9990234375\n",
            "\n",
            "\n",
            "Epoch:  65\n",
            "training accuracy:  0.9921875\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.994921875\n",
            "training accuracy:  0.9953125\n",
            "eval accuracy:  0.998828125\n",
            "\n",
            "\n",
            "Epoch:  66\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.9953125\n",
            "eval accuracy:  0.9990234375\n",
            "\n",
            "\n",
            "Epoch:  67\n",
            "training accuracy:  0.99765625\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.996484375\n",
            "eval accuracy:  0.998828125\n",
            "\n",
            "\n",
            "Epoch:  68\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.99375\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.994921875\n",
            "eval accuracy:  0.998828125\n",
            "\n",
            "\n",
            "Epoch:  69\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.99375\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.99375\n",
            "eval accuracy:  0.9990234375\n",
            "\n",
            "\n",
            "Epoch:  70\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.997265625\n",
            "eval accuracy:  0.9986328125\n",
            "\n",
            "\n",
            "Epoch:  71\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.9984375\n",
            "eval accuracy:  0.998828125\n",
            "\n",
            "\n",
            "Epoch:  72\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.994921875\n",
            "training accuracy:  0.99765625\n",
            "eval accuracy:  0.9984375\n",
            "\n",
            "\n",
            "Epoch:  73\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.995703125\n",
            "eval accuracy:  0.99921875\n",
            "\n",
            "\n",
            "Epoch:  74\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.99375\n",
            "eval accuracy:  0.9986328125\n",
            "\n",
            "\n",
            "Epoch:  75\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.994921875\n",
            "training accuracy:  0.998046875\n",
            "training accuracy:  0.99765625\n",
            "eval accuracy:  0.998828125\n",
            "\n",
            "\n",
            "Epoch:  76\n",
            "training accuracy:  0.99296875\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.998046875\n",
            "eval accuracy:  0.99921875\n",
            "\n",
            "\n",
            "Epoch:  77\n",
            "training accuracy:  0.9984375\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.99765625\n",
            "eval accuracy:  0.9990234375\n",
            "\n",
            "\n",
            "Epoch:  78\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.99375\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.998046875\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.995703125\n",
            "eval accuracy:  0.998828125\n",
            "\n",
            "\n",
            "Epoch:  79\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.99765625\n",
            "training accuracy:  0.994140625\n",
            "eval accuracy:  0.9978515625\n",
            "\n",
            "\n",
            "Epoch:  80\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.996875\n",
            "eval accuracy:  0.9990234375\n",
            "\n",
            "\n",
            "Epoch:  81\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.99375\n",
            "training accuracy:  0.994921875\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.99375\n",
            "training accuracy:  0.997265625\n",
            "eval accuracy:  0.9990234375\n",
            "\n",
            "\n",
            "Epoch:  82\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.99765625\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.99609375\n",
            "eval accuracy:  0.998828125\n",
            "\n",
            "\n",
            "Epoch:  83\n",
            "training accuracy:  0.9984375\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.99765625\n",
            "training accuracy:  0.996875\n",
            "eval accuracy:  0.9982421875\n",
            "\n",
            "\n",
            "Epoch:  84\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.9953125\n",
            "eval accuracy:  0.9990234375\n",
            "\n",
            "\n",
            "Epoch:  85\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.99765625\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.994921875\n",
            "eval accuracy:  0.998828125\n",
            "learning rate 2e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEYCAYAAAA6b7/5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy:  0.9802827380952381 eval accuracy:  0.9829126213592233 ( use_bn_trainstat= False )\n",
            "\n",
            "\n",
            "Epoch:  86\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.997265625\n",
            "eval accuracy:  0.998046875\n",
            "\n",
            "\n",
            "Epoch:  87\n",
            "training accuracy:  0.99765625\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.99609375\n",
            "eval accuracy:  0.99921875\n",
            "\n",
            "\n",
            "Epoch:  88\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.993359375\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.997265625\n",
            "eval accuracy:  0.9982421875\n",
            "\n",
            "\n",
            "Epoch:  89\n",
            "training accuracy:  0.995703125\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.99765625\n",
            "training accuracy:  0.997265625\n",
            "eval accuracy:  0.998828125\n",
            "\n",
            "\n",
            "Epoch:  90\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.997265625\n",
            "eval accuracy:  0.9990234375\n",
            "\n",
            "\n",
            "Epoch:  91\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.997265625\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.99453125\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.99765625\n",
            "training accuracy:  0.996875\n",
            "eval accuracy:  0.998828125\n",
            "\n",
            "\n",
            "Epoch:  92\n",
            "training accuracy:  0.996875\n",
            "training accuracy:  0.996484375\n",
            "training accuracy:  0.99609375\n",
            "training accuracy:  0.9984375\n",
            "training accuracy:  0.994140625\n",
            "training accuracy:  0.9953125\n",
            "training accuracy:  0.99609375\n",
            "eval accuracy:  0.9984375\n",
            "learning rate 2e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEYCAYAAAA6b7/5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngP8HDCnmqFVc1lYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy:  0.9825148809523809 eval accuracy:  0.9844660194174757 ( use_bn_trainstat= False )\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}